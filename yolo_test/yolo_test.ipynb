{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f648c546",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2ab4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure matplotlib for inline display in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "\n",
    "\n",
    "print('‚úì Libraries imported successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f8f7d4",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f75a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directories\n",
    "BASE_DIR = Path.cwd().parent\n",
    "MODEL_NAME = \"yolov8l\"  # Model name without .pt extension\n",
    "\n",
    "# Choose YOLO model versions that are fully supported with ultralytics:\n",
    "# ‚úÖ YOLOv8: 'yolov8n', 'yolov8s', 'yolov8m', 'yolov8l', 'yolov8x'\n",
    "# ‚úÖ YOLOv9: 'yolov9s', 'yolov9m', 'yolov9l', 'yolov9x'\n",
    "# ‚úÖ YOLOv10: 'yolov10n', 'yolov10s', 'yolov10m', 'yolov10l', 'yolov10x'\n",
    "# ‚úÖ YOLO11: 'yolo11n', 'yolo11s', 'yolo11m', 'yolo11l', 'yolo11x'\n",
    "# ‚úÖ YOLO12: 'yolo12n', 'yolo12s', 'yolo12m', 'yolo12l', 'yolo12x'\n",
    "#\n",
    "# Model sizes: n=nano, s=small, m=medium, l=large, x=extra-large\n",
    "\n",
    "MODELS_DIR = BASE_DIR / 'models' / MODEL_NAME\n",
    "TMP_DIR = BASE_DIR / 'tmp' / MODEL_NAME\n",
    "RUNS_DIR = BASE_DIR / 'quick_test' / 'runs'\n",
    "\n",
    "# Dataset Selection - Choose one:\n",
    "# Option 1: Full dataset (~100k images)\n",
    "# YOLO_DATASET_ROOT = BASE_DIR / 'bdd100k_yolo'\n",
    "# DATA_YAML_PATH = YOLO_DATASET_ROOT / 'data.yaml'\n",
    "\n",
    "# Option 2: Limited dataset (representative samples - for quick testing)\n",
    "YOLO_DATASET_ROOT = BASE_DIR / 'bdd100k_yolo_limited'\n",
    "DATA_YAML_PATH = YOLO_DATASET_ROOT / 'data.yaml'\n",
    "\n",
    "# Choose data split\n",
    "USED_DATA_SPLIT = \"test\"  # 'train', 'val', or 'test'\n",
    "\n",
    "# Dataset paths\n",
    "IMAGES_DIR = YOLO_DATASET_ROOT / 'images' / USED_DATA_SPLIT\n",
    "LABELS_DIR = YOLO_DATASET_ROOT / 'labels' / USED_DATA_SPLIT\n",
    "\n",
    "# Verify dataset exists\n",
    "if not DATA_YAML_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset not found: {DATA_YAML_PATH}\\n\\n\"\n",
    "        f\"Please run the dataset preparation script first:\\n\"\n",
    "        f\"  python3 process_bdd100k_to_yolo_dataset.py\\n\"\n",
    "    )\n",
    "\n",
    "# Generate timestamp and run name\n",
    "RUN_TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "RUN_NAME = f'{MODEL_NAME}_{YOLO_DATASET_ROOT.name}_{USED_DATA_SPLIT}'\n",
    "\n",
    "# Create run-specific directory\n",
    "RUN_DIR = RUNS_DIR / RUN_NAME\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TMP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load class names from data.yaml\n",
    "import yaml\n",
    "with open(DATA_YAML_PATH, 'r') as f:\n",
    "    dataset_config = yaml.safe_load(f)\n",
    "    CLASS_NAMES = {i: name for i, name in enumerate(dataset_config['names'])}\n",
    "\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "# Generate colors for each class\n",
    "np.random.seed(42)\n",
    "COLORS = {idx: tuple(np.random.randint(50, 255, 3).tolist()) for idx in CLASS_NAMES.keys()}\n",
    "\n",
    "print('‚úì Configuration loaded')\n",
    "print(f'  Model: {MODEL_NAME}')\n",
    "print(f'  Dataset: {YOLO_DATASET_ROOT.name}')\n",
    "print(f'  Split: {USED_DATA_SPLIT}')\n",
    "print(f'  Images: {IMAGES_DIR}')\n",
    "print(f'  Labels: {LABELS_DIR}')\n",
    "print(f'  Classes: {NUM_CLASSES}')\n",
    "print(f'  Run directory: {RUN_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff817251",
   "metadata": {},
   "source": [
    "## 3. Load YOLO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270c04c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLO model with automatic download\n",
    "model_path = MODELS_DIR / f'{MODEL_NAME}.pt'\n",
    "\n",
    "if not model_path.exists():\n",
    "    print(f'Model not found at {model_path}')\n",
    "    print(f'Downloading {MODEL_NAME} ...')\n",
    "    \n",
    "    try:\n",
    "        # Download model - it will be cached by ultralytics\n",
    "        MODEL_NAME_n = MODEL_NAME \n",
    "        if MODEL_NAME.startswith('yolov11') or MODEL_NAME.startswith('yolov12'):\n",
    "            MODEL_NAME_n = MODEL_NAME + '.pt'\n",
    "        model = YOLO(MODEL_NAME_n)\n",
    "        \n",
    "        # Create models directory\n",
    "        MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save model to our directory using export/save\n",
    "        try:\n",
    "            # Try to save using the model's save method\n",
    "            if hasattr(model, 'save'):\n",
    "                model.save(str(model_path))\n",
    "                print(f'‚úì Model downloaded and saved to {model_path}')\n",
    "                print(f'  Size: {model_path.stat().st_size / (1024*1024):.1f} MB')\n",
    "            else:\n",
    "                # Fallback: copy from cache\n",
    "                import glob\n",
    "                cache_patterns = [\n",
    "                    str(Path.home() / '.cache' / 'ultralytics' / '**' / f'{MODEL_NAME}.pt'),\n",
    "                    str(Path.home() / '.config' / 'Ultralytics' / '**' / f'{MODEL_NAME}.pt'),\n",
    "                ]\n",
    "                \n",
    "                model_found = False\n",
    "                for pattern in cache_patterns:\n",
    "                    cache_paths = glob.glob(pattern, recursive=True)\n",
    "                    if cache_paths:\n",
    "                        shutil.copy(cache_paths[0], model_path)\n",
    "                        print(f'‚úì Model downloaded and saved to {model_path}')\n",
    "                        print(f'  Size: {model_path.stat().st_size / (1024*1024):.1f} MB')\n",
    "                        model_found = True\n",
    "                        break\n",
    "                \n",
    "                if not model_found:\n",
    "                    print(f'‚úì Model loaded from ultralytics cache')\n",
    "                    print(f'  Note: Model is in cache, not copied to {model_path}')\n",
    "                    print(f'  This is normal and the model will work correctly')\n",
    "        except Exception as save_error:\n",
    "            print(f'‚ö†Ô∏è  Could not save model to custom location: {save_error}')\n",
    "            print(f'‚úì Model loaded successfully from ultralytics cache')\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f'\\n‚ùå Error downloading model: {e}')\n",
    "        raise\n",
    "else:\n",
    "    model = YOLO(str(model_path))\n",
    "    print(f'‚úì Model loaded from {model_path}')\n",
    "\n",
    "# Get model information\n",
    "model_params = sum(p.numel() for p in model.model.parameters())\n",
    "model_size_mb = model_path.stat().st_size / (1024*1024) if model_path.exists() else 0\n",
    "\n",
    "# Calculate FLOPs using model.info() method which is more accurate\n",
    "try:\n",
    "    from ultralytics.utils.torch_utils import get_flops\n",
    "    # FLOPs for 640x640 input (standard YOLO input size)\n",
    "    flops = get_flops(model.model, imgsz=(1, 3, 640, 640))\n",
    "    flops_gflops = flops / 1e9\n",
    "except Exception as e:\n",
    "    print(f'‚ö†Ô∏è  Could not calculate FLOPs: {e}')\n",
    "    flops_gflops = 0\n",
    "\n",
    "print(f'\\nüìä Model Information:')\n",
    "print(f'  Model: {MODEL_NAME}')\n",
    "print(f'  Classes in model: {len(model.names)}')\n",
    "print(f'  Task: {model.task}')\n",
    "print(f'  Parameters: {model_params / 1e6:.1f}M')\n",
    "print(f'  Model Size: {model_size_mb:.1f} MB')\n",
    "print(f'  FLOPs (640x640): {flops_gflops:.2f} GFLOPs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f25df55",
   "metadata": {},
   "source": [
    "## 4. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c63218",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Get all images and labels\n",
    "image_files = sorted(list(IMAGES_DIR.glob('*.jpg')) + list(IMAGES_DIR.glob('*.png')))\n",
    "label_files = sorted([LABELS_DIR / f'{img.stem}.txt' for img in image_files if (LABELS_DIR / f'{img.stem}.txt').exists()])\n",
    "\n",
    "# Filter to only images with labels\n",
    "valid_images = [img for img in image_files if (LABELS_DIR / f'{img.stem}.txt').exists()]\n",
    "\n",
    "print(f'‚úì Dataset loaded')\n",
    "print(f'  Total images: {len(image_files)}')\n",
    "print(f'  Images with labels: {len(valid_images)}')\n",
    "print(f'  Label files: {len(label_files)}')\n",
    "\n",
    "# Load performance metadata for per-image attribute analysis\n",
    "METADATA_DIR = YOLO_DATASET_ROOT / 'representative_json'\n",
    "PERFORMANCE_FILE = METADATA_DIR / f'{USED_DATA_SPLIT}_performance_analysis.json'\n",
    "\n",
    "if PERFORMANCE_FILE.exists():\n",
    "    with open(PERFORMANCE_FILE, 'r') as f:\n",
    "        performance_data = json.load(f)\n",
    "    \n",
    "    print(f'\\n‚úì Performance metadata loaded: {PERFORMANCE_FILE.name}')\n",
    "    print(f'  Images with attributes: {performance_data[\"total_images\"]}')\n",
    "    print(f'  Attributes available: weather, scene, timeofday')\n",
    "    print(f'  Per-image class distribution available')\n",
    "    \n",
    "    # Create quick lookup dictionary for attributes\n",
    "    image_attributes = {img['basename']: img for img in performance_data['images']}\n",
    "else:\n",
    "    print(f'\\n‚ö†Ô∏è  Performance metadata not found: {PERFORMANCE_FILE}')\n",
    "    print(f'  Attribute-based analysis will not be available')\n",
    "    print(f'  Run: python3 process_bdd100k_to_yolo_dataset.py')\n",
    "    performance_data = None\n",
    "    image_attributes = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552ca01b",
   "metadata": {},
   "source": [
    "## 6. Run Official YOLO Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1251769",
   "metadata": {},
   "source": [
    "### IMPORTANT: Optimized Validation Approach\n",
    "This notebook uses **YOLO's official validation method** to calculate all metrics, confusion matrix, and predictions in a single pass. This approach:\n",
    "- ‚úÖ **Faster**: Single validation pass instead of multiple loops\n",
    "- ‚úÖ **No Duplicates**: Uses YOLO's built-in validation logic\n",
    "- ‚úÖ **Official Metrics**: Provides mAP, precision, recall directly from YOLO\n",
    "- ‚úÖ **Confusion Matrix**: Extracted from YOLO validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8ec6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run official YOLO validation with W&B tracking\n",
    "import time\n",
    "print('=' * 80)\n",
    "print('RUNNING OFFICIAL YOLO VALIDATION WITH W&B TRACKING')\n",
    "print('=' * 80)\n",
    "\n",
    "# Create a dataset structure that YOLO expects\n",
    "validation_dataset_root = TMP_DIR / 'yolo_validation_dataset'\n",
    "validation_images_dir = validation_dataset_root / 'images' / USED_DATA_SPLIT\n",
    "validation_labels_dir = validation_dataset_root / 'labels' / USED_DATA_SPLIT\n",
    "\n",
    "# Create directories\n",
    "validation_images_dir.mkdir(parents=True, exist_ok=True)\n",
    "validation_labels_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create symbolic links to images and labels\n",
    "import shutil\n",
    "print(f'Setting up validation dataset structure...')\n",
    "\n",
    "# Link images\n",
    "for img_file in IMAGES_DIR.glob('*.jpg'):\n",
    "    link_path = validation_images_dir / img_file.name\n",
    "    if not link_path.exists():\n",
    "        try:\n",
    "            link_path.symlink_to(img_file)\n",
    "        except:\n",
    "            shutil.copy2(img_file, link_path)\n",
    "\n",
    "for img_file in IMAGES_DIR.glob('*.png'):\n",
    "    link_path = validation_images_dir / img_file.name\n",
    "    if not link_path.exists():\n",
    "        try:\n",
    "            link_path.symlink_to(img_file)\n",
    "        except:\n",
    "            shutil.copy2(img_file, link_path)\n",
    "\n",
    "# Link labels\n",
    "for label_file in LABELS_DIR.glob('*.txt'):\n",
    "    if label_file.name != 'classes.txt':\n",
    "        link_path = validation_labels_dir / label_file.name\n",
    "        if not link_path.exists():\n",
    "            try:\n",
    "                link_path.symlink_to(label_file)\n",
    "            except:\n",
    "                shutil.copy2(label_file, link_path)\n",
    "\n",
    "print(f'‚úì Validation dataset prepared')\n",
    "print(f'  Images: {len(list(validation_images_dir.glob(\"*\")))}')\n",
    "print(f'  Labels: {len(list(validation_labels_dir.glob(\"*.txt\")))}')\n",
    "\n",
    "# Create data.yaml file\n",
    "data_yaml_path = validation_dataset_root / 'data.yaml'\n",
    "iou_threshold = 0.5\n",
    "data_yaml_content = f\"\"\"path: {validation_dataset_root}\n",
    "train: images/train\n",
    "val: images/{USED_DATA_SPLIT}\n",
    "test: images/test\n",
    "\n",
    "nc: {NUM_CLASSES}\n",
    "names: {list(CLASS_NAMES.values())}\n",
    "\"\"\"\n",
    "\n",
    "with open(data_yaml_path, 'w') as f:\n",
    "    f.write(data_yaml_content)\n",
    "\n",
    "print(f'‚úì Created data.yaml at: {data_yaml_path}')\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "try:\n",
    "    import wandb\n",
    "    wandb.init(\n",
    "        project=\"yolo-bdd100k-validation\",\n",
    "        name=W_B_RUN_NAME,\n",
    "        config={\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"dataset\": USED_DATASET,\n",
    "            \"split\": USED_DATA_SPLIT,\n",
    "            \"iou_threshold\": iou_threshold,\n",
    "            \"num_classes\": NUM_CLASSES,\n",
    "            \"model_params\": model_params,\n",
    "            \"model_size_mb\": model_size_mb,\n",
    "            \"flops_gflops\": flops_gflops\n",
    "        }\n",
    "    )\n",
    "    print(f'\\n‚úì Weights & Biases initialized: {W_B_RUN_NAME}')\n",
    "    print(f'  Project: yolo-bdd100k-validation')\n",
    "    print(f'  Run: {W_B_RUN_NAME}')\n",
    "except ImportError:\n",
    "    print('\\n‚ö†Ô∏è  Weights & Biases not available. Install with: pip install wandb')\n",
    "    wandb = None\n",
    "except Exception as e:\n",
    "    print(f'\\n‚ö†Ô∏è  W&B initialization error: {e}')\n",
    "    print('  Continuing without W&B tracking...')\n",
    "    wandb = None\n",
    "\n",
    "# Run validation with timing\n",
    "print('\\nRunning YOLO validation...')\n",
    "start_time = time.time()\n",
    "\n",
    "validation_results = model.val(\n",
    "    data=str(data_yaml_path),\n",
    "    split=USED_DATA_SPLIT,\n",
    "    save_json=False,\n",
    "    save_txt=False,\n",
    "    conf=0.001,\n",
    "    iou=iou_threshold,\n",
    "    verbose=True,\n",
    "    plots=True,\n",
    "    project=str(RUN_DIR),\n",
    "    name='yolo_validation'\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "# Extract speed metrics from YOLO validation\n",
    "num_images = len(list(validation_images_dir.glob(\"*.jpg\"))) + len(list(validation_images_dir.glob(\"*.png\")))\n",
    "avg_inference_time = total_time / num_images if num_images > 0 else 0\n",
    "fps = 1 / avg_inference_time if avg_inference_time > 0 else 0\n",
    "\n",
    "# Extract overall metrics\n",
    "yolo_metrics = {\n",
    "    'precision': float(validation_results.box.mp),\n",
    "    'recall': float(validation_results.box.mr),\n",
    "    'map50': float(validation_results.box.map50),\n",
    "    'map50_95': float(validation_results.box.map),\n",
    "    'fitness': float(validation_results.fitness)\n",
    "}\n",
    "\n",
    "# Extract per-class metrics from YOLO\n",
    "yolo_class_metrics = {}\n",
    "class_tp = {}\n",
    "class_fp = {}\n",
    "class_fn = {}\n",
    "\n",
    "if hasattr(validation_results.box, 'ap_class_index') and len(validation_results.box.ap_class_index) > 0:\n",
    "    for i, class_idx in enumerate(validation_results.box.ap_class_index):\n",
    "        class_idx = int(class_idx)\n",
    "        class_name = CLASS_NAMES.get(class_idx, f'class_{class_idx}')\n",
    "        \n",
    "        precision = float(validation_results.box.p[i]) if i < len(validation_results.box.p) else 0.0\n",
    "        recall = float(validation_results.box.r[i]) if i < len(validation_results.box.r) else 0.0\n",
    "        ap50 = float(validation_results.box.ap50[i]) if i < len(validation_results.box.ap50) else 0.0\n",
    "        ap50_95 = float(validation_results.box.ap[i]) if i < len(validation_results.box.ap) else 0.0\n",
    "        \n",
    "        yolo_class_metrics[class_name] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'ap50': ap50,\n",
    "            'ap50_95': ap50_95\n",
    "        }\n",
    "        \n",
    "        class_tp[class_idx] = 0\n",
    "        class_fp[class_idx] = 0\n",
    "        class_fn[class_idx] = 0\n",
    "\n",
    "# Extract confusion matrix from YOLO validation results\n",
    "confusion_matrix = validation_results.confusion_matrix.matrix if hasattr(validation_results, 'confusion_matrix') else np.zeros((NUM_CLASSES, NUM_CLASSES), dtype=int)\n",
    "\n",
    "# Verify confusion matrix exists\n",
    "if confusion_matrix is not None and confusion_matrix.size > 0:\n",
    "    print(f'\\n‚úì Confusion matrix extracted successfully')\n",
    "    print(f'  Shape: {confusion_matrix.shape}')\n",
    "    print(f'  Diagonal sum (correct): {np.trace(confusion_matrix)}')\n",
    "else:\n",
    "    print(f'\\n‚ö†Ô∏è  Warning: Confusion matrix is empty')\n",
    "\n",
    "# Calculate TP, FP, FN from confusion matrix\n",
    "for i in range(NUM_CLASSES):\n",
    "    class_tp[i] = int(confusion_matrix[i, i]) if i < confusion_matrix.shape[0] and i < confusion_matrix.shape[1] else 0\n",
    "    \n",
    "    if i < confusion_matrix.shape[1]:\n",
    "        class_fp[i] = int(confusion_matrix[:, i].sum() - confusion_matrix[i, i])\n",
    "    else:\n",
    "        class_fp[i] = 0\n",
    "    \n",
    "    if i < confusion_matrix.shape[0]:\n",
    "        class_fn[i] = int(confusion_matrix[i, :].sum() - confusion_matrix[i, i])\n",
    "    else:\n",
    "        class_fn[i] = 0\n",
    "\n",
    "# Create results_data for compatibility with report generation\n",
    "results_data = []\n",
    "for img_path in validation_images_dir.glob(\"*.jpg\"):\n",
    "    results_data.append({'image_path': img_path})\n",
    "for img_path in validation_images_dir.glob(\"*.png\"):\n",
    "    results_data.append({'image_path': img_path})\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('OFFICIAL YOLO VALIDATION RESULTS')\n",
    "print('=' * 80)\n",
    "print(f\"Precision (mean): {yolo_metrics['precision']:.4f}\")\n",
    "print(f\"Recall (mean):    {yolo_metrics['recall']:.4f}\")\n",
    "print(f\"mAP@0.5:          {yolo_metrics['map50']:.4f}\")\n",
    "print(f\"mAP@0.5:0.95:     {yolo_metrics['map50_95']:.4f}\")\n",
    "print(f\"Fitness:          {yolo_metrics['fitness']:.4f}\")\n",
    "print(f'\\n‚ö° Performance Metrics:')\n",
    "print(f'  Total Time: {total_time:.2f}s')\n",
    "print(f'  Average Inference Time: {avg_inference_time*1000:.2f}ms per image')\n",
    "print(f'  FPS (Frames Per Second): {fps:.2f}')\n",
    "print('=' * 80)\n",
    "\n",
    "# Log comprehensive metrics to Weights & Biases\n",
    "if wandb:\n",
    "    try:\n",
    "        # Log overall metrics\n",
    "        wandb.log({\n",
    "            # Accuracy metrics\n",
    "            \"metrics/precision\": yolo_metrics['precision'],\n",
    "            \"metrics/recall\": yolo_metrics['recall'],\n",
    "            \"metrics/mAP@0.5\": yolo_metrics['map50'],\n",
    "            \"metrics/mAP@0.5:0.95\": yolo_metrics['map50_95'],\n",
    "            \"metrics/fitness\": yolo_metrics['fitness'],\n",
    "            \n",
    "            # Performance metrics\n",
    "            \"performance/total_time_seconds\": total_time,\n",
    "            \"performance/avg_inference_time_ms\": avg_inference_time * 1000,\n",
    "            \"performance/fps\": fps,\n",
    "            \"performance/images_processed\": num_images,\n",
    "            \n",
    "            # Model info\n",
    "            \"model/parameters_millions\": model_params / 1e6,\n",
    "            \"model/size_mb\": model_size_mb,\n",
    "            \"model/flops_gflops\": flops_gflops\n",
    "        })\n",
    "        \n",
    "        # Log per-class metrics\n",
    "        for class_name, metrics in yolo_class_metrics.items():\n",
    "            wandb.log({\n",
    "                f\"class/{class_name}/precision\": metrics['precision'],\n",
    "                f\"class/{class_name}/recall\": metrics['recall'],\n",
    "                f\"class/{class_name}/ap50\": metrics['ap50'],\n",
    "                f\"class/{class_name}/ap50_95\": metrics['ap50_95']\n",
    "            })\n",
    "        \n",
    "        print(f'\\n‚úì Metrics logged to Weights & Biases')\n",
    "        print(f'  Dashboard: https://wandb.ai/yolo-bdd100k-validation/{W_B_RUN_NAME}')\n",
    "    except Exception as e:\n",
    "        print(f'\\n‚ö†Ô∏è  Error logging to W&B: {e}')\n",
    "\n",
    "# Save YOLO validation results with performance metrics\n",
    "yolo_results_path = RUN_DIR / 'yolo_validation_metrics.json'\n",
    "import json\n",
    "with open(yolo_results_path, 'w') as f:\n",
    "    json.dump({\n",
    "        'overall_metrics': yolo_metrics,\n",
    "        'per_class_metrics': yolo_class_metrics,\n",
    "        'performance': {\n",
    "            'total_time_seconds': float(total_time),\n",
    "            'avg_inference_time_ms': float(avg_inference_time * 1000),\n",
    "            'fps': float(fps),\n",
    "            'images_processed': int(num_images)\n",
    "        },\n",
    "        'model_info': {\n",
    "            'parameters': int(model_params),\n",
    "            'model_size_mb': float(model_size_mb),\n",
    "            'flops_gflops': float(flops_gflops)\n",
    "        }\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f'\\n‚úì YOLO validation results saved to: {yolo_results_path}')\n",
    "print(f'‚úì YOLO validation plots saved to: {RUN_DIR / \"yolo_validation\"}')\n",
    "print(f'‚úì Confusion matrix extracted and will be visualized in next cell')\n",
    "print(f'  Diagonal sum (correct predictions): {np.trace(confusion_matrix)}')\n",
    "print(f'  Total predictions: {confusion_matrix.sum()}')\n",
    "\n",
    "# Finish W&B run\n",
    "if wandb:\n",
    "    try:\n",
    "        wandb.finish()\n",
    "        print(f'\\n‚úì Weights & Biases run completed successfully')\n",
    "    except Exception as e:\n",
    "        print(f'\\n‚ö†Ô∏è  Error finishing W&B run: {e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60078000",
   "metadata": {},
   "source": [
    "## 7. Display Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addec244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-class metrics from YOLO validation results\n",
    "metrics_data = []\n",
    "\n",
    "for class_id in sorted(CLASS_NAMES.keys()):\n",
    "    tp = class_tp.get(class_id, 0)\n",
    "    fp = class_fp.get(class_id, 0)\n",
    "    fn = class_fn.get(class_id, 0)\n",
    "    \n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    metrics_data.append({\n",
    "        'Class': CLASS_NAMES[class_id],\n",
    "        'TP': tp,\n",
    "        'FP': fp,\n",
    "        'FN': fn,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1\n",
    "    })\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Overall metrics\n",
    "total_tp = sum(class_tp.values())\n",
    "total_fp = sum(class_fp.values())\n",
    "total_fn = sum(class_fn.values())\n",
    "\n",
    "overall_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "overall_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "overall_f1 = 2 * overall_precision * overall_recall / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0\n",
    "\n",
    "print('=' * 80)\n",
    "print('YOLOv8 VALIDATION RESULTS SUMMARY')\n",
    "print('=' * 80)\n",
    "print(f'\\\\nDataset: BDD100K Limited - Validation Split')\n",
    "print(f'Images processed: {len(list(validation_images_dir.glob(\"*\")))}')\n",
    "print(f'IoU Threshold: {iou_threshold}')\n",
    "print(f'\\\\nOVERALL METRICS (From YOLO Validation):')\n",
    "print(f'  Precision: {yolo_metrics[\"precision\"]:.4f}')\n",
    "print(f'  Recall:    {yolo_metrics[\"recall\"]:.4f}')\n",
    "print(f'  mAP@0.5:   {yolo_metrics[\"map50\"]:.4f}')\n",
    "print(f'  mAP@0.5:0.95: {yolo_metrics[\"map50_95\"]:.4f}')\n",
    "print(f'\\\\nOVERALL METRICS (From Confusion Matrix):')\n",
    "print(f'  Precision: {overall_precision:.4f}')\n",
    "print(f'  Recall:    {overall_recall:.4f}')\n",
    "print(f'  F1-Score:  {overall_f1:.4f}')\n",
    "print(f'\\\\nPER-CLASS METRICS:')\n",
    "print(df_metrics.to_string(index=False))\n",
    "print('\\\\n' + '=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccba6091",
   "metadata": {},
   "source": [
    "## 8. Visualize Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76936a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot metrics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# Precision by class\n",
    "ax1 = axes[0, 0]\n",
    "df_metrics_sorted = df_metrics.sort_values('Precision', ascending=True)\n",
    "ax1.barh(df_metrics_sorted['Class'], df_metrics_sorted['Precision'], color='skyblue')\n",
    "ax1.set_xlabel('Precision', fontweight='bold')\n",
    "ax1.set_title('Precision by Class', fontweight='bold', fontsize=14)\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Recall by class\n",
    "ax2 = axes[0, 1]\n",
    "df_metrics_sorted = df_metrics.sort_values('Recall', ascending=True)\n",
    "ax2.barh(df_metrics_sorted['Class'], df_metrics_sorted['Recall'], color='lightcoral')\n",
    "ax2.set_xlabel('Recall', fontweight='bold')\n",
    "ax2.set_title('Recall by Class', fontweight='bold', fontsize=14)\n",
    "ax2.set_xlim([0, 1])\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# F1-Score by class\n",
    "ax3 = axes[1, 0]\n",
    "df_metrics_sorted = df_metrics.sort_values('F1-Score', ascending=True)\n",
    "ax3.barh(df_metrics_sorted['Class'], df_metrics_sorted['F1-Score'], color='lightgreen')\n",
    "ax3.set_xlabel('F1-Score', fontweight='bold')\n",
    "ax3.set_title('F1-Score by Class', fontweight='bold', fontsize=14)\n",
    "ax3.set_xlim([0, 1])\n",
    "ax3.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Detection counts\n",
    "ax4 = axes[1, 1]\n",
    "counts = [class_tp[i] + class_fn[i] for i in sorted(CLASS_NAMES.keys())]\n",
    "class_labels = [CLASS_NAMES[i] for i in sorted(CLASS_NAMES.keys())]\n",
    "ax4.bar(class_labels, counts, color='mediumpurple')\n",
    "ax4.set_xlabel('Class', fontweight='bold')\n",
    "ax4.set_ylabel('Total Objects (TP + FN)', fontweight='bold')\n",
    "ax4.set_title('Ground Truth Object Counts', fontweight='bold', fontsize=14)\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save for report\n",
    "metrics_fig_path = RUN_DIR / 'metrics_charts.png'\n",
    "plt.savefig(metrics_fig_path, dpi=150, bbox_inches='tight')\n",
    "\n",
    "# Show in notebook\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602394be",
   "metadata": {},
   "source": [
    "## 8.5. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a90f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix from YOLO validation\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Create larger figure\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 14))\n",
    "\n",
    "# Draw each cell manually with solid colors\n",
    "for i in range(NUM_CLASSES):\n",
    "    for j in range(NUM_CLASSES):\n",
    "        value = confusion_matrix[i, j]\n",
    "        \n",
    "        # Determine cell color\n",
    "        if value == 0:\n",
    "            # White for empty cells\n",
    "            cell_color = 'white'\n",
    "        elif i == j:\n",
    "            # Green for diagonal (True Positives)\n",
    "            cell_color = '#00CC00'\n",
    "        else:\n",
    "            # Red for off-diagonal (False Positives/Negatives)\n",
    "            cell_color = '#E60000'\n",
    "        \n",
    "        # Draw cell as rectangle\n",
    "        rect = Rectangle((j - 0.5, i - 0.5), 1, 1, \n",
    "                         facecolor=cell_color, \n",
    "                         edgecolor='black', \n",
    "                         linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add text annotations\n",
    "        if value > 0:\n",
    "            ax.text(j, i, str(value), ha='center', va='center', \n",
    "                   color='white', fontsize=18, fontweight='bold')\n",
    "\n",
    "# Set axis limits and properties\n",
    "ax.set_xlim(-0.5, NUM_CLASSES - 0.5)\n",
    "ax.set_ylim(NUM_CLASSES - 0.5, -0.5)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# Set ticks and labels\n",
    "class_labels = [CLASS_NAMES[i] for i in range(NUM_CLASSES)]\n",
    "ax.set_xticks(np.arange(NUM_CLASSES))\n",
    "ax.set_yticks(np.arange(NUM_CLASSES))\n",
    "ax.set_xticklabels(class_labels, fontsize=14, fontweight='bold')\n",
    "ax.set_yticklabels(class_labels, fontsize=14, fontweight='bold')\n",
    "\n",
    "# Rotate x labels for better readability\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha='right', rotation_mode='anchor')\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Predicted Class', fontweight='bold', fontsize=16, labelpad=10)\n",
    "ax.set_ylabel('True Class', fontweight='bold', fontsize=16, labelpad=10)\n",
    "ax.set_title(f'Confusion Matrix (From {MODEL_NAME} Validation)\\\\n(Green = Correct Predictions, Red = Incorrect Predictions, White = No Predictions)', \n",
    "             fontweight='bold', fontsize=18, pad=20)\n",
    "\n",
    "# Remove all gridlines\n",
    "ax.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save confusion matrix figure for PDF report\n",
    "confusion_matrix_path = RUN_DIR / 'confusion_matrix.png'\n",
    "plt.savefig(confusion_matrix_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'‚úì Confusion matrix visualized (from {MODEL_NAME} validation)')\n",
    "print(f'  Diagonal sum (correct predictions): {np.trace(confusion_matrix)}')\n",
    "print(f'  Total predictions: {confusion_matrix.sum()}')\n",
    "print(f'  Saved to: {confusion_matrix_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3361edcf",
   "metadata": {},
   "source": [
    "## 9. Detailed Comparison: Ground Truth vs Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66a2262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_ground_truth(img_path, label_path, class_names, colors):\n",
    "    \"\"\"Draw ground truth boxes on image.\"\"\"\n",
    "    img = cv2.imread(str(img_path))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    h, w = img.shape[:2]\n",
    "    \n",
    "    if not label_path.exists():\n",
    "        return img, 0\n",
    "    \n",
    "    object_count = 0\n",
    "    with open(label_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 5:\n",
    "                object_count += 1\n",
    "                class_id = int(parts[0])\n",
    "                x_center, y_center, width, height = map(float, parts[1:5])\n",
    "                \n",
    "                # Convert to pixel coordinates\n",
    "                x1 = int((x_center - width / 2) * w)\n",
    "                y1 = int((y_center - height / 2) * h)\n",
    "                x2 = int((x_center + width / 2) * w)\n",
    "                y2 = int((y_center + height / 2) * h)\n",
    "                \n",
    "                # Draw box\n",
    "                color = colors.get(class_id, (255, 255, 255))\n",
    "                cv2.rectangle(img, (x1, y1), (x2, y2), color, 3)\n",
    "                \n",
    "                # Draw label\n",
    "                label = class_names.get(class_id, f'class_{class_id}')\n",
    "                (label_w, label_h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n",
    "                cv2.rectangle(img, (x1, y1 - label_h - 10), (x1 + label_w + 10, y1), color, -1)\n",
    "                cv2.putText(img, label, (x1 + 5, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "    \n",
    "    return img, object_count\n",
    "\n",
    "\n",
    "# Run predictions on sample images for detailed comparison\n",
    "print('=' * 80)\n",
    "print('GENERATING DETAILED GROUND TRUTH VS PREDICTIONS COMPARISON')\n",
    "print('=' * 80)\n",
    "\n",
    "# Create directory for comparison images\n",
    "comparisons_dir = RUN_DIR / 'sample_comparisons'\n",
    "comparisons_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Select 12 random samples\n",
    "num_comparisons = min(12, len(valid_images))\n",
    "comparison_indices = random.sample(range(len(valid_images)), num_comparisons)\n",
    "sample_images = [valid_images[i] for i in comparison_indices]\n",
    "\n",
    "# Run predictions on sample images\n",
    "print(f'\\nRunning predictions on {num_comparisons} sample images...')\n",
    "sample_results = []\n",
    "for img_path in tqdm(sample_images, desc='Processing samples'):\n",
    "    result = model(str(img_path), verbose=False)[0]\n",
    "    sample_results.append({\n",
    "        'image_path': img_path,\n",
    "        'result': result,\n",
    "        'num_detections': len(result.boxes)\n",
    "    })\n",
    "\n",
    "# Generate 12 separate comparison images (side-by-side)\n",
    "comparison_image_paths = []\n",
    "for idx, sample_data in enumerate(sample_results, 1):\n",
    "    img_path = sample_data['image_path']\n",
    "    label_path = LABELS_DIR / f'{img_path.stem}.txt'\n",
    "    result = sample_data['result']\n",
    "    \n",
    "    # Ground truth\n",
    "    gt_img, gt_count = draw_ground_truth(img_path, label_path, CLASS_NAMES, COLORS)\n",
    "    \n",
    "    # Predictions\n",
    "    pred_img = result.plot()\n",
    "    pred_img_rgb = cv2.cvtColor(pred_img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Create side-by-side figure for this pair\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Ground truth\n",
    "    axes[0].imshow(gt_img)\n",
    "    axes[0].set_title(\n",
    "        f'Ground Truth\\n{img_path.name}\\n({gt_count} objects)', \n",
    "        fontweight='bold', fontsize=14, pad=15\n",
    "    )\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Predictions\n",
    "    axes[1].imshow(pred_img_rgb)\n",
    "    axes[1].set_title(\n",
    "        f'Predictions\\n{img_path.name}\\n({sample_data[\"num_detections\"]} objects)',\n",
    "        fontweight='bold', fontsize=14, pad=15\n",
    "    )\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Add overall title\n",
    "    fig.suptitle(f'Comparison #{idx}: Ground Truth vs YOLO Predictions', \n",
    "                 fontsize=18, fontweight='bold', y=0.98)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save individual comparison\n",
    "    comparison_path = comparisons_dir / f'comparison_{idx:02d}.png'\n",
    "    plt.savefig(comparison_path, dpi=150, bbox_inches='tight')\n",
    "    comparison_image_paths.append(comparison_path)\n",
    "    \n",
    "    # Show in notebook\n",
    "    plt.show()\n",
    "    \n",
    "    # Close figure to free memory\n",
    "    plt.close(fig)\n",
    "\n",
    "print(f'\\n‚úì Generated {num_comparisons} individual comparison images')\n",
    "print(f'  Saved to: {comparisons_dir}')\n",
    "print(f'  Each image is high-resolution side-by-side comparison')\n",
    "print('=' * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f32527",
   "metadata": {},
   "source": [
    "## 10. Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66da26d3",
   "metadata": {},
   "source": [
    "## 9.5 Attribute-Based Performance Analysis\n",
    "\n",
    "Analyze model performance across different environmental conditions and scenes using the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e16951",
   "metadata": {},
   "outputs": [],
   "source": [
    "if performance_data and image_attributes:\n",
    "    print(\"=\"*80)\n",
    "    print(\"ATTRIBUTE-BASED PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Group results by attributes\n",
    "    results_by_weather = {}\n",
    "    results_by_scene = {}\n",
    "    results_by_timeofday = {}\n",
    "    results_by_class = {}\n",
    "    \n",
    "    for result in results_data:\n",
    "        basename = Path(result['image_path']).stem\n",
    "        \n",
    "        # Get attributes for this image\n",
    "        attrs = image_attributes.get(basename, {})\n",
    "        weather = attrs.get('weather', 'unknown')\n",
    "        scene = attrs.get('scene', 'unknown')\n",
    "        timeofday = attrs.get('timeofday', 'unknown')\n",
    "        classes_present = attrs.get('classes_present', [])\n",
    "        \n",
    "        # Group by weather\n",
    "        if weather not in results_by_weather:\n",
    "            results_by_weather[weather] = []\n",
    "        results_by_weather[weather].append(result)\n",
    "        \n",
    "        # Group by scene\n",
    "        if scene not in results_by_scene:\n",
    "            results_by_scene[scene] = []\n",
    "        results_by_scene[scene].append(result)\n",
    "        \n",
    "        # Group by time of day\n",
    "        if timeofday not in results_by_timeofday:\n",
    "            results_by_timeofday[timeofday] = []\n",
    "        results_by_timeofday[timeofday].append(result)\n",
    "        \n",
    "        # Group by classes present in ground truth\n",
    "        for cls in classes_present:\n",
    "            if cls not in results_by_class:\n",
    "                results_by_class[cls] = []\n",
    "            results_by_class[cls].append(result)\n",
    "    \n",
    "    # Calculate metrics for each group\n",
    "    def calculate_group_metrics(group_results):\n",
    "        \"\"\"Calculate average metrics for a group of results\"\"\"\n",
    "        if not group_results:\n",
    "            return None\n",
    "        \n",
    "        total_tp = sum(r['true_positives'] for r in group_results)\n",
    "        total_fp = sum(r['false_positives'] for r in group_results)\n",
    "        total_fn = sum(r['false_negatives'] for r in group_results)\n",
    "        \n",
    "        precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "        recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'count': len(group_results),\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'tp': total_tp,\n",
    "            'fp': total_fp,\n",
    "            'fn': total_fn\n",
    "        }\n",
    "    \n",
    "    # Display weather-based performance\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PERFORMANCE BY WEATHER CONDITION\")\n",
    "    print(\"=\"*80)\n",
    "    weather_metrics = {}\n",
    "    for weather, group in sorted(results_by_weather.items()):\n",
    "        metrics = calculate_group_metrics(group)\n",
    "        if metrics:\n",
    "            weather_metrics[weather] = metrics\n",
    "            print(f\"\\n{weather.upper()}:\")\n",
    "            print(f\"  Images: {metrics['count']}\")\n",
    "            print(f\"  Precision: {metrics['precision']:.3f}\")\n",
    "            print(f\"  Recall: {metrics['recall']:.3f}\")\n",
    "            print(f\"  F1 Score: {metrics['f1']:.3f}\")\n",
    "    \n",
    "    # Display scene-based performance\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PERFORMANCE BY SCENE TYPE\")\n",
    "    print(\"=\"*80)\n",
    "    scene_metrics = {}\n",
    "    for scene, group in sorted(results_by_scene.items()):\n",
    "        metrics = calculate_group_metrics(group)\n",
    "        if metrics:\n",
    "            scene_metrics[scene] = metrics\n",
    "            print(f\"\\n{scene.upper()}:\")\n",
    "            print(f\"  Images: {metrics['count']}\")\n",
    "            print(f\"  Precision: {metrics['precision']:.3f}\")\n",
    "            print(f\"  Recall: {metrics['recall']:.3f}\")\n",
    "            print(f\"  F1 Score: {metrics['f1']:.3f}\")\n",
    "    \n",
    "    # Display time-based performance\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PERFORMANCE BY TIME OF DAY\")\n",
    "    print(\"=\"*80)\n",
    "    timeofday_metrics = {}\n",
    "    for timeofday, group in sorted(results_by_timeofday.items()):\n",
    "        metrics = calculate_group_metrics(group)\n",
    "        if metrics:\n",
    "            timeofday_metrics[timeofday] = metrics\n",
    "            print(f\"\\n{timeofday.upper()}:\")\n",
    "            print(f\"  Images: {metrics['count']}\")\n",
    "            print(f\"  Precision: {metrics['precision']:.3f}\")\n",
    "            print(f\"  Recall: {metrics['recall']:.3f}\")\n",
    "            print(f\"  F1 Score: {metrics['f1']:.3f}\")\n",
    "    \n",
    "    # Display class-based performance\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"PERFORMANCE BY CLASS (Top 10 by image count)\")\n",
    "    print(\"=\"*80)\n",
    "    class_metrics = {}\n",
    "    for cls, group in results_by_class.items():\n",
    "        metrics = calculate_group_metrics(group)\n",
    "        if metrics:\n",
    "            class_metrics[cls] = metrics\n",
    "    \n",
    "    # Sort by image count and show top 10\n",
    "    top_classes = sorted(class_metrics.items(), key=lambda x: x[1]['count'], reverse=True)[:10]\n",
    "    for cls, metrics in top_classes:\n",
    "        print(f\"\\n{cls.upper()}:\")\n",
    "        print(f\"  Images: {metrics['count']}\")\n",
    "        print(f\"  Precision: {metrics['precision']:.3f}\")\n",
    "        print(f\"  Recall: {metrics['recall']:.3f}\")\n",
    "        print(f\"  F1 Score: {metrics['f1']:.3f}\")\n",
    "    \n",
    "    # Visualize attribute-based performance\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Weather performance\n",
    "    ax = axes[0, 0]\n",
    "    weather_names = list(weather_metrics.keys())\n",
    "    weather_f1 = [weather_metrics[w]['f1'] for w in weather_names]\n",
    "    ax.barh(weather_names, weather_f1, color='skyblue')\n",
    "    ax.set_xlabel('F1 Score')\n",
    "    ax.set_title('Performance by Weather Condition')\n",
    "    ax.set_xlim(0, 1)\n",
    "    \n",
    "    # Scene performance\n",
    "    ax = axes[0, 1]\n",
    "    scene_names = list(scene_metrics.keys())\n",
    "    scene_f1 = [scene_metrics[s]['f1'] for s in scene_names]\n",
    "    ax.barh(scene_names, scene_f1, color='lightcoral')\n",
    "    ax.set_xlabel('F1 Score')\n",
    "    ax.set_title('Performance by Scene Type')\n",
    "    ax.set_xlim(0, 1)\n",
    "    \n",
    "    # Time of day performance\n",
    "    ax = axes[1, 0]\n",
    "    time_names = list(timeofday_metrics.keys())\n",
    "    time_f1 = [timeofday_metrics[t]['f1'] for t in time_names]\n",
    "    ax.barh(time_names, time_f1, color='lightgreen')\n",
    "    ax.set_xlabel('F1 Score')\n",
    "    ax.set_title('Performance by Time of Day')\n",
    "    ax.set_xlim(0, 1)\n",
    "    \n",
    "    # Top classes performance\n",
    "    ax = axes[1, 1]\n",
    "    class_names_top = [c[0] for c in top_classes[:8]]  # Top 8 for readability\n",
    "    class_f1_top = [c[1]['f1'] for c in top_classes[:8]]\n",
    "    ax.barh(class_names_top, class_f1_top, color='plum')\n",
    "    ax.set_xlabel('F1 Score')\n",
    "    ax.set_title('Performance by Class (Top 8)')\n",
    "    ax.set_xlim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save figure\n",
    "    attribute_perf_path = RUN_DIR / 'attribute_performance.png'\n",
    "    plt.savefig(attribute_perf_path, dpi=150, bbox_inches='tight')\n",
    "    print(f\"\\n‚úì Saved attribute performance visualization: {attribute_perf_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"‚úì ATTRIBUTE-BASED ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "else:\n",
    "    print(\"=\"*80)\n",
    "    print(\"‚ö†Ô∏è  Attribute-based analysis not available\")\n",
    "    print(\"Performance metadata not loaded\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaa975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from reportlab.lib.pagesizes import letter, A4\n",
    "from reportlab.lib import colors\n",
    "from reportlab.lib.units import inch\n",
    "from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer, Image, PageBreak\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.enums import TA_CENTER, TA_LEFT\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "# Generate comprehensive PDF report with all visualizations\n",
    "pdf_report_path = RUN_DIR / 'report.pdf'\n",
    "\n",
    "# Create PDF document\n",
    "doc = SimpleDocTemplate(str(pdf_report_path), pagesize=A4,\n",
    "                       rightMargin=30, leftMargin=30,\n",
    "                       topMargin=30, bottomMargin=30)\n",
    "\n",
    "# Container for PDF elements\n",
    "story = []\n",
    "styles = getSampleStyleSheet()\n",
    "\n",
    "# Custom styles\n",
    "title_style = ParagraphStyle(\n",
    "    'CustomTitle',\n",
    "    parent=styles['Heading1'],\n",
    "    fontSize=24,\n",
    "    textColor=colors.HexColor('#2c3e50'),\n",
    "    spaceAfter=30,\n",
    "    alignment=TA_CENTER\n",
    ")\n",
    "\n",
    "heading_style = ParagraphStyle(\n",
    "    'CustomHeading',\n",
    "    parent=styles['Heading2'],\n",
    "    fontSize=16,\n",
    "    textColor=colors.HexColor('#34495e'),\n",
    "    spaceAfter=12,\n",
    "    spaceBefore=20\n",
    ")\n",
    "\n",
    "# Title\n",
    "story.append(Paragraph('YOLO Validation Report', title_style))\n",
    "story.append(Spacer(1, 12))\n",
    "\n",
    "# Info section with model details\n",
    "info_data = [\n",
    "    ['Model:', MODEL_NAME],\n",
    "    ['Model Size:', f'{model_size_mb:.1f} MB'],\n",
    "    ['Parameters:', f'{model_params / 1e6:.1f}M'],\n",
    "    ['FLOPs (640x640):', f'{flops_gflops:.2f} GFLOPs'],\n",
    "    ['Run Name:', RUN_NAME],\n",
    "    ['W&B Run Name:', W_B_RUN_NAME],\n",
    "    ['Timestamp:', datetime.now().strftime('%Y-%m-%d %H:%M:%S')],\n",
    "    ['Dataset:', f'{USED_DATASET} - {USED_DATA_SPLIT} split'],\n",
    "    ['Images Processed:', str(len(results_data))],\n",
    "    ['IoU Threshold:', str(iou_threshold)]\n",
    "]\n",
    "\n",
    "info_table = Table(info_data, colWidths=[2.2*inch, 3.8*inch])\n",
    "info_table.setStyle(TableStyle([\n",
    "    ('BACKGROUND', (0, 0), (-1, -1), colors.HexColor('#ecf0f1')),\n",
    "    ('TEXTCOLOR', (0, 0), (-1, -1), colors.black),\n",
    "    ('ALIGN', (0, 0), (-1, -1), 'LEFT'),\n",
    "    ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),\n",
    "    ('FONTSIZE', (0, 0), (-1, -1), 10),\n",
    "    ('BOTTOMPADDING', (0, 0), (-1, -1), 8),\n",
    "    ('TOPPADDING', (0, 0), (-1, -1), 8),\n",
    "    ('GRID', (0, 0), (-1, -1), 1, colors.white)\n",
    "]))\n",
    "story.append(info_table)\n",
    "story.append(Spacer(1, 20))\n",
    "\n",
    "# Performance metrics section\n",
    "story.append(Paragraph('Inference Performance', heading_style))\n",
    "perf_data = [\n",
    "    ['Metric', 'Value'],\n",
    "    ['Total Execution Time', f'{total_time:.2f}s'],\n",
    "    ['Average Inference Time', f'{avg_inference_time*1000:.2f}ms per image'],\n",
    "    ['FPS (Frames Per Second)', f'{fps:.2f}']\n",
    "]\n",
    "\n",
    "perf_table = Table(perf_data, colWidths=[3*inch, 3*inch])\n",
    "perf_table.setStyle(TableStyle([\n",
    "    ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#27ae60')),\n",
    "    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n",
    "    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n",
    "    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
    "    ('FONTSIZE', (0, 0), (-1, 0), 12),\n",
    "    ('FONTSIZE', (0, 1), (-1, -1), 10),\n",
    "    ('BOTTOMPADDING', (0, 0), (-1, -1), 8),\n",
    "    ('TOPPADDING', (0, 0), (-1, -1), 8),\n",
    "    ('BACKGROUND', (0, 1), (-1, -1), colors.HexColor('#d5f4e6')),\n",
    "    ('GRID', (0, 0), (-1, -1), 1, colors.black)\n",
    "]))\n",
    "story.append(perf_table)\n",
    "story.append(Spacer(1, 20))\n",
    "\n",
    "# Overall accuracy metrics\n",
    "story.append(Paragraph('Overall Accuracy Metrics', heading_style))\n",
    "acc_data = [\n",
    "    ['Metric', 'Value'],\n",
    "    ['Precision', f'{overall_precision:.4f}'],\n",
    "    ['Recall', f'{overall_recall:.4f}'],\n",
    "    ['F1-Score', f'{overall_f1:.4f}'],\n",
    "    ['mAP@0.5', f'{yolo_metrics[\"map50\"]:.4f}'],\n",
    "    ['mAP@0.5:0.95', f'{yolo_metrics[\"map50_95\"]:.4f}'],\n",
    "    ['True Positives', str(total_tp)],\n",
    "    ['False Positives', str(total_fp)],\n",
    "    ['False Negatives', str(total_fn)]\n",
    "]\n",
    "\n",
    "acc_table = Table(acc_data, colWidths=[3*inch, 3*inch])\n",
    "acc_table.setStyle(TableStyle([\n",
    "    ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#3498db')),\n",
    "    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n",
    "    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n",
    "    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
    "    ('FONTSIZE', (0, 0), (-1, 0), 12),\n",
    "    ('FONTSIZE', (0, 1), (-1, -1), 10),\n",
    "    ('BOTTOMPADDING', (0, 0), (-1, -1), 8),\n",
    "    ('TOPPADDING', (0, 0), (-1, -1), 8),\n",
    "    ('BACKGROUND', (0, 1), (-1, -1), colors.beige),\n",
    "    ('GRID', (0, 0), (-1, -1), 1, colors.black)\n",
    "]))\n",
    "story.append(acc_table)\n",
    "story.append(Spacer(1, 20))\n",
    "\n",
    "# Add metrics visualization to PDF\n",
    "story.append(PageBreak())\n",
    "story.append(Paragraph('Performance Visualizations', heading_style))\n",
    "story.append(Spacer(1, 12))\n",
    "\n",
    "metrics_fig_path = RUN_DIR / 'metrics_charts.png'\n",
    "if metrics_fig_path.exists():\n",
    "    try:\n",
    "        with PILImage.open(metrics_fig_path) as img:\n",
    "            img_width, img_height = img.size\n",
    "            aspect_ratio = img_height / img_width\n",
    "            pdf_width = 7 * inch\n",
    "            pdf_height = pdf_width * aspect_ratio\n",
    "            story.append(Image(str(metrics_fig_path), width=pdf_width, height=pdf_height))\n",
    "    except Exception as e:\n",
    "        print(f'Warning: Could not load metrics chart with PIL: {e}')\n",
    "        story.append(Image(str(metrics_fig_path), width=7*inch, height=5*inch))\n",
    "else:\n",
    "    story.append(Paragraph('Metrics chart not found.', styles['Normal']))\n",
    "\n",
    "story.append(Spacer(1, 20))\n",
    "\n",
    "# Per-class metrics table\n",
    "story.append(PageBreak())\n",
    "story.append(Paragraph('Per-Class Performance', heading_style))\n",
    "\n",
    "table_data = [['Class', 'TP', 'FP', 'FN', 'Precision', 'Recall', 'F1-Score', 'mAP@0.5']]\n",
    "for _, row in df_metrics.iterrows():\n",
    "    class_name = row['Class']\n",
    "    map50_val = yolo_class_metrics.get(class_name, {}).get('ap50', 0.0)\n",
    "    table_data.append([\n",
    "        row['Class'],\n",
    "        str(row['TP']),\n",
    "        str(row['FP']),\n",
    "        str(row['FN']),\n",
    "        f\"{row['Precision']:.4f}\",\n",
    "        f\"{row['Recall']:.4f}\",\n",
    "        f\"{row['F1-Score']:.4f}\",\n",
    "        f\"{map50_val:.4f}\"\n",
    "    ])\n",
    "\n",
    "per_class_table = Table(table_data, colWidths=[1.0*inch, 0.5*inch, 0.5*inch, 0.5*inch, 0.8*inch, 0.8*inch, 0.8*inch, 0.8*inch])\n",
    "per_class_table.setStyle(TableStyle([\n",
    "    ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#3498db')),\n",
    "    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n",
    "    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n",
    "    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
    "    ('FONTSIZE', (0, 0), (-1, 0), 8),\n",
    "    ('FONTSIZE', (0, 1), (-1, -1), 7),\n",
    "    ('BOTTOMPADDING', (0, 0), (-1, -1), 6),\n",
    "    ('TOPPADDING', (0, 0), (-1, -1), 6),\n",
    "    ('ROWBACKGROUNDS', (0, 1), (-1, -1), [colors.white, colors.lightgrey]),\n",
    "    ('GRID', (0, 0), (-1, -1), 1, colors.black)\n",
    "]))\n",
    "story.append(per_class_table)\n",
    "story.append(Spacer(1, 20))\n",
    "\n",
    "# Confusion Matrix\n",
    "story.append(PageBreak())\n",
    "story.append(Paragraph('Confusion Matrix', heading_style))\n",
    "story.append(Paragraph(f'Correct Predictions (Diagonal Sum): {np.trace(confusion_matrix)}', styles['Normal']))\n",
    "story.append(Paragraph(f'Total Matched Predictions: {confusion_matrix.sum()}', styles['Normal']))\n",
    "story.append(Spacer(1, 12))\n",
    "\n",
    "confusion_matrix_img_path = RUN_DIR / 'confusion_matrix.png'\n",
    "if confusion_matrix_img_path.exists():\n",
    "    try:\n",
    "        with PILImage.open(confusion_matrix_img_path) as img:\n",
    "            img_width, img_height = img.size\n",
    "            aspect_ratio = img_height / img_width\n",
    "            pdf_width = 6.5 * inch\n",
    "            pdf_height = pdf_width * aspect_ratio\n",
    "            if pdf_height > 7 * inch:\n",
    "                pdf_height = 7 * inch\n",
    "                pdf_width = pdf_height / aspect_ratio\n",
    "            story.append(Image(str(confusion_matrix_img_path), width=pdf_width, height=pdf_height))\n",
    "    except Exception as e:\n",
    "        print(f'Warning: Could not load confusion matrix with PIL: {e}')\n",
    "        story.append(Image(str(confusion_matrix_img_path), width=6.5*inch, height=6.5*inch))\n",
    "else:\n",
    "    story.append(Paragraph('Confusion matrix image not found.', styles['Normal']))\n",
    "\n",
    "story.append(Spacer(1, 20))\n",
    "\n",
    "# Sample predictions comparison - multiple images per page\n",
    "comparisons_dir = RUN_DIR / 'sample_comparisons'\n",
    "if comparisons_dir.exists():\n",
    "    comparison_files = sorted(comparisons_dir.glob('comparison_*.png'))\n",
    "    \n",
    "    if comparison_files:\n",
    "        story.append(PageBreak())\n",
    "        story.append(Paragraph('Sample Predictions: Ground Truth vs Predictions', heading_style))\n",
    "        story.append(Paragraph(f'{len(comparison_files)} detailed side-by-side comparisons', styles['Normal']))\n",
    "        story.append(Spacer(1, 12))\n",
    "        \n",
    "        # Add comparison images - 2 per page with page breaks between pairs\n",
    "        for i, comparison_path in enumerate(comparison_files):\n",
    "            # Add page break after every 3 images (except before the first)\n",
    "            if i > 0 and i % 3 == 0:\n",
    "                story.append(PageBreak())\n",
    "            \n",
    "            try:\n",
    "                with PILImage.open(comparison_path) as img:\n",
    "                    img_width, img_height = img.size\n",
    "                    aspect_ratio = img_height / img_width\n",
    "                    \n",
    "                    # Smaller width to fit 2 per page\n",
    "                    pdf_width = 7 * inch\n",
    "                    pdf_height = pdf_width * aspect_ratio\n",
    "                    \n",
    "                    # Limit height to fit 2 images per page\n",
    "                    if pdf_height > 3.5 * inch:\n",
    "                        pdf_height = 3.5 * inch\n",
    "                        pdf_width = pdf_height / aspect_ratio\n",
    "                    \n",
    "                    story.append(Image(str(comparison_path), width=pdf_width, height=pdf_height))\n",
    "                    story.append(Spacer(1, 8))\n",
    "            except Exception as e:\n",
    "                print(f'Warning: Could not load {comparison_path.name} with PIL: {e}')\n",
    "                story.append(Image(str(comparison_path), width=7*inch, height=3*inch))\n",
    "                story.append(Spacer(1, 8))\n",
    "    else:\n",
    "        story.append(PageBreak())\n",
    "        story.append(Paragraph('Sample Predictions: Ground Truth vs Predictions', heading_style))\n",
    "        story.append(Paragraph('No comparison images found.', styles['Normal']))\n",
    "else:\n",
    "    story.append(PageBreak())\n",
    "    story.append(Paragraph('Sample Predictions: Ground Truth vs Predictions', heading_style))\n",
    "    story.append(Paragraph('Comparison directory not found.', styles['Normal']))\n",
    "\n",
    "story.append(Spacer(1, 12))\n",
    "story.append(Paragraph('Additional validation plots available in: yolo_validation folder', styles['Normal']))\n",
    "\n",
    "# Footer\n",
    "story.append(Spacer(1, 30))\n",
    "story.append(Paragraph('Generated by YOLO Quick Test Notebook', \n",
    "                      ParagraphStyle('Footer', parent=styles['Normal'], \n",
    "                                   alignment=TA_CENTER, textColor=colors.grey)))\n",
    "story.append(Paragraph('BDD100K Dataset - Computer Vision Project', \n",
    "                      ParagraphStyle('Footer2', parent=styles['Normal'], \n",
    "                                   alignment=TA_CENTER, textColor=colors.grey)))\n",
    "\n",
    "# Build PDF\n",
    "doc.build(story)\n",
    "\n",
    "# Generate JSON file with comprehensive comparison data\n",
    "json_report_path = RUN_DIR / 'metrics_data.json'\n",
    "\n",
    "comparison_data = {\n",
    "    'metadata': {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'run_name': RUN_NAME,\n",
    "        'wb_run_name': W_B_RUN_NAME,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'dataset': USED_DATASET,\n",
    "        'data_split': USED_DATA_SPLIT,\n",
    "        'images_processed': len(results_data),\n",
    "        'iou_threshold': iou_threshold,\n",
    "        'num_classes': NUM_CLASSES\n",
    "    },\n",
    "    'model_info': {\n",
    "        'parameters': int(model_params),\n",
    "        'model_size_mb': float(model_size_mb),\n",
    "        'flops_gflops': float(flops_gflops)\n",
    "    },\n",
    "    'performance': {\n",
    "        'total_time_seconds': float(total_time),\n",
    "        'avg_inference_time_ms': float(avg_inference_time * 1000),\n",
    "        'fps': float(fps),\n",
    "        'images_processed': int(len(results_data))\n",
    "    },\n",
    "    'custom_metrics': {\n",
    "        'overall': {\n",
    "            'precision': float(overall_precision),\n",
    "            'recall': float(overall_recall),\n",
    "            'f1_score': float(overall_f1),\n",
    "            'true_positives': int(total_tp),\n",
    "            'false_positives': int(total_fp),\n",
    "            'false_negatives': int(total_fn)\n",
    "        },\n",
    "        'per_class': {}\n",
    "    },\n",
    "    'yolo_official_metrics': {\n",
    "        'overall': yolo_metrics,\n",
    "        'per_class': yolo_class_metrics\n",
    "    },\n",
    "    'confusion_matrix': {\n",
    "        'matrix': confusion_matrix.tolist(),\n",
    "        'diagonal_sum': int(np.trace(confusion_matrix)),\n",
    "        'total_predictions': int(confusion_matrix.sum())\n",
    "    },\n",
    "    'class_names': CLASS_NAMES\n",
    "}\n",
    "\n",
    "# Add per-class custom metrics\n",
    "for _, row in df_metrics.iterrows():\n",
    "    class_name = row['Class']\n",
    "    comparison_data['custom_metrics']['per_class'][class_name] = {\n",
    "        'true_positives': int(row['TP']),\n",
    "        'false_positives': int(row['FP']),\n",
    "        'false_negatives': int(row['FN']),\n",
    "        'precision': float(row['Precision']),\n",
    "        'recall': float(row['Recall']),\n",
    "        'f1_score': float(row['F1-Score'])\n",
    "    }\n",
    "\n",
    "# Save JSON file\n",
    "import json\n",
    "with open(json_report_path, 'w') as f:\n",
    "    json.dump(comparison_data, f, indent=2)\n",
    "\n",
    "# Count comparison images\n",
    "comparisons_dir = RUN_DIR / 'sample_comparisons'\n",
    "num_comparison_images = len(list(comparisons_dir.glob('comparison_*.png'))) if comparisons_dir.exists() else 0\n",
    "\n",
    "# Summary output\n",
    "print('=' * 80)\n",
    "print('‚úì COMPREHENSIVE REPORT GENERATED')\n",
    "print('=' * 80)\n",
    "print(f'\\nAll outputs saved to: {RUN_DIR}')\n",
    "print(f'\\nGenerated files:')\n",
    "print(f'  üìÑ PDF Report: report.pdf ({pdf_report_path.stat().st_size / 1024:.2f} KB)')\n",
    "print(f'  üìä Metrics JSON: metrics_data.json ({json_report_path.stat().st_size / 1024:.2f} KB)')\n",
    "print(f'  üìà YOLO Validation: yolo_validation_metrics.json')\n",
    "print(f'  üñºÔ∏è  Confusion Matrix: confusion_matrix.png')\n",
    "print(f'  üìä Metrics Charts: metrics_charts.png')\n",
    "print(f'  üñºÔ∏è  Sample Comparisons: sample_comparisons/ ({num_comparison_images} images)')\n",
    "print(f'  üìÅ YOLO Outputs: yolo_validation/ (plots, curves, etc.)')\n",
    "print(f'\\nReport contents:')\n",
    "print(f'  - Model Information (Size: {model_size_mb:.1f}MB, Params: {model_params/1e6:.1f}M, FLOPs: {flops_gflops:.2f}G)')\n",
    "print(f'  - Performance Metrics (FPS: {fps:.2f}, Avg: {avg_inference_time*1000:.2f}ms)')\n",
    "print(f'  - Overall accuracy metrics (Precision, Recall, F1, mAP)')\n",
    "print(f'  - Per-class performance with IoU metrics')\n",
    "print(f'  - Performance visualizations (4 charts)')\n",
    "print(f'  - Confusion matrix visualization')\n",
    "print(f'  - {num_comparison_images} individual side-by-side prediction comparisons (full-width, high-resolution)')\n",
    "print(f'  - Custom & YOLO official metrics')\n",
    "print(f'\\nüíæ Metrics saved to JSON for future comparison')\n",
    "\n",
    "print(f'üìä Weights & Biases tracking: {W_B_RUN_NAME}')\n",
    "print('=' * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
