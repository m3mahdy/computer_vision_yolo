{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "481feb87",
   "metadata": {},
   "source": [
    "# YOLO Model Testing and Evaluation\n",
    "\n",
    "This notebook provides comprehensive testing and evaluation of trained YOLO models on the BDD100K dataset.\n",
    "\n",
    "## Features:\n",
    "- ‚úÖ Load fine-tuned models\n",
    "- ‚úÖ Run inference on test set\n",
    "- ‚úÖ Comprehensive metrics (mAP, precision, recall, F1)\n",
    "- ‚úÖ Per-class performance analysis\n",
    "- ‚úÖ Confusion matrix visualization\n",
    "- ‚úÖ Speed benchmarking (FPS, latency)\n",
    "- ‚úÖ Sample predictions visualization\n",
    "- ‚úÖ PDF report generation\n",
    "\n",
    "## Workflow:\n",
    "1. Import libraries and configuration\n",
    "2. Load fine-tuned model\n",
    "3. Verify test dataset\n",
    "4. Run model evaluation\n",
    "5. Analyze performance metrics\n",
    "6. Generate visualizations\n",
    "7. Create comprehensive PDF report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f648c546",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2ab4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries (uncomment if running in Colab)\n",
    "# !pip install -q ultralytics pyyaml reportlab pillow\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import json\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import glob\n",
    "from typing import Dict, Tuple, Any\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# YOLO imports\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils.torch_utils import get_flops\n",
    "\n",
    "# PDF generation imports\n",
    "from reportlab.lib.pagesizes import letter, A4\n",
    "from reportlab.lib import colors\n",
    "from reportlab.lib.units import inch\n",
    "from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer, Image, PageBreak\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.lib.enums import TA_CENTER, TA_LEFT\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure matplotlib for notebook display\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "\n",
    "# Check GPU availability\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('‚úì Libraries imported successfully')\n",
    "print(f'‚úì Device: {device}')\n",
    "if device == 'cuda':\n",
    "    print(f'  GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'  CUDA Version: {torch.version.cuda}')\n",
    "    print(f'  Available Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f8f7d4",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f75a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directories\n",
    "BASE_DIR = Path.cwd().parent\n",
    "\n",
    "# from pathlib import Path\n",
    "# BASE_DIR = Path(\"/computer_vision_yolo\")\n",
    "\n",
    "\n",
    "MODEL_NAME = \"yolov8n\"  # Model name without .pt extension\n",
    "\n",
    "# Choose YOLO model versions that are fully supported with ultralytics:\n",
    "# ‚úÖ YOLOv8: 'yolov8n', 'yolov8s', 'yolov8m', 'yolov8l', 'yolov8x'\n",
    "# ‚úÖ YOLOv9: 'yolov9s', 'yolov9m', 'yolov9l', 'yolov9x'\n",
    "# ‚úÖ YOLOv10: 'yolov10n', 'yolov10s', 'yolov10m', 'yolov10l', 'yolov10x'\n",
    "# ‚úÖ YOLO11: 'yolo11n', 'yolo11s', 'yolo11m', 'yolo11l', 'yolo11x'\n",
    "# ‚úÖ YOLO12: 'yolo12n', 'yolo12s', 'yolo12m', 'yolo12l', 'yolo12x'\n",
    "#\n",
    "# Model sizes: n=nano, s=small, m=medium, l=large, x=extra-large\n",
    "\n",
    "MODELS_DIR = BASE_DIR / 'models' / MODEL_NAME\n",
    "TMP_DIR = BASE_DIR / 'tmp' / MODEL_NAME\n",
    "RUNS_DIR = BASE_DIR / 'yolo_test' / 'runs'\n",
    "\n",
    "# Dataset Selection - Choose one:\n",
    "# Option 1: Full dataset (~100k images)\n",
    "# YOLO_DATASET_ROOT = BASE_DIR / 'bdd100k_yolo'\n",
    "# DATA_YAML_PATH = YOLO_DATASET_ROOT / 'data.yaml'\n",
    "\n",
    "# Option 2: Limited dataset (representative samples - for quick testing)\n",
    "YOLO_DATASET_ROOT = BASE_DIR / 'bdd100k_yolo_limited'\n",
    "DATA_YAML_PATH = YOLO_DATASET_ROOT / 'data.yaml'\n",
    "\n",
    "# Choose data split\n",
    "USED_DATA_SPLIT = \"test\"  # 'train', 'val', or 'test'\n",
    "\n",
    "# Dataset paths\n",
    "IMAGES_DIR = YOLO_DATASET_ROOT / 'images' / USED_DATA_SPLIT\n",
    "LABELS_DIR = YOLO_DATASET_ROOT / 'labels' / USED_DATA_SPLIT\n",
    "\n",
    "# Verify dataset exists\n",
    "if not DATA_YAML_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset not found: {DATA_YAML_PATH}\\n\\n\"\n",
    "        f\"Please run the dataset preparation script first:\\n\"\n",
    "        f\"  python3 process_bdd100k_to_yolo_dataset.py\\n\"\n",
    "    )\n",
    "\n",
    "USED_DATASET = YOLO_DATASET_ROOT.name\n",
    "RUN_TIMESTAMP = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "RUN_NAME = f'{MODEL_NAME}_{YOLO_DATASET_ROOT.name}_{USED_DATA_SPLIT}'\n",
    "W_B_RUN_NAME = f'{MODEL_NAME}_{USED_DATASET}_{USED_DATA_SPLIT}_{RUN_TIMESTAMP}'\n",
    "\n",
    "# Create run-specific directory\n",
    "RUN_DIR = RUNS_DIR / RUN_NAME\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "TMP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load class names and build deterministic lookups\n",
    "with open(DATA_YAML_PATH, 'r') as f:\n",
    "    dataset_config = yaml.safe_load(f)\n",
    "\n",
    "raw_names = dataset_config.get('names', {})\n",
    "if isinstance(raw_names, dict):\n",
    "    CLASS_NAMES = {int(class_id): name for class_id, name in raw_names.items()}\n",
    "elif isinstance(raw_names, list):\n",
    "    CLASS_NAMES = {idx: name for idx, name in enumerate(raw_names)}\n",
    "else:\n",
    "    raise ValueError('Unsupported class name structure in data.yaml')\n",
    "\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "COLORS = {\n",
    "    class_id: tuple(int(channel) for channel in rng.integers(40, 255, size=3))\n",
    "    for class_id in CLASS_NAMES.keys()\n",
    "}\n",
    "CLASS_NAME_TO_ID = {name: class_id for class_id, name in CLASS_NAMES.items()}\n",
    "\n",
    "print('=' * 80)\n",
    "print('CONFIGURATION SUMMARY')\n",
    "print('=' * 80)\n",
    "print(f'Model: {MODEL_NAME}')\n",
    "print(f'Dataset: {YOLO_DATASET_ROOT.name}')\n",
    "print(f'Split: {USED_DATA_SPLIT}')\n",
    "print(f'Classes: {NUM_CLASSES}')\n",
    "print(f'Class Names: {list(CLASS_NAMES.values())}')\n",
    "print(f'Device: {device}')\n",
    "print(f'Images dir: {IMAGES_DIR}')\n",
    "print(f'Labels dir: {LABELS_DIR}')\n",
    "print(f'Run directory: {RUN_DIR}')\n",
    "print('=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff817251",
   "metadata": {},
   "source": [
    "## 3. Load YOLO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270c04c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLO model with automatic download\n",
    "model_path = MODELS_DIR / f'{MODEL_NAME}.pt'\n",
    "\n",
    "if not model_path.exists():\n",
    "    print(f'Model not found at {model_path}')\n",
    "    print(f'Downloading {MODEL_NAME} ...')\n",
    "    \n",
    "    try:\n",
    "        # Download model - it will be cached by ultralytics\n",
    "        MODEL_NAME_n = MODEL_NAME \n",
    "        if MODEL_NAME.startswith('yolov11') or MODEL_NAME.startswith('yolov12'):\n",
    "            MODEL_NAME_n = MODEL_NAME + '.pt'\n",
    "        model = YOLO(MODEL_NAME_n)\n",
    "        \n",
    "        # Create models directory\n",
    "        MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save model to our directory using export/save\n",
    "        try:\n",
    "            # Try to save using the model's save method\n",
    "            if hasattr(model, 'save'):\n",
    "                model.save(str(model_path))\n",
    "                print(f'‚úì Model downloaded and saved to {model_path}')\n",
    "                print(f'  Size: {model_path.stat().st_size / (1024*1024):.1f} MB')\n",
    "            else:\n",
    "                # Fallback: copy from cache\n",
    "                cache_patterns = [\n",
    "                    str(Path.home() / '.cache' / 'ultralytics' / '**' / f'{MODEL_NAME}.pt'),\n",
    "                    str(Path.home() / '.config' / 'Ultralytics' / '**' / f'{MODEL_NAME}.pt'),\n",
    "                ]\n",
    "                \n",
    "                model_found = False\n",
    "                for pattern in cache_patterns:\n",
    "                    cache_paths = glob.glob(pattern, recursive=True)\n",
    "                    if cache_paths:\n",
    "                        shutil.copy(cache_paths[0], model_path)\n",
    "                        print(f'‚úì Model downloaded and saved to {model_path}')\n",
    "                        print(f'  Size: {model_path.stat().st_size / (1024*1024):.1f} MB')\n",
    "                        model_found = True\n",
    "                        break\n",
    "                \n",
    "                if not model_found:\n",
    "                    print(f'‚úì Model loaded from ultralytics cache')\n",
    "                    print(f'  Note: Model is in cache, not copied to {model_path}')\n",
    "                    print(f'  This is normal and the model will work correctly')\n",
    "        except Exception as save_error:\n",
    "            print(f'‚ö†Ô∏è  Could not save model to custom location: {save_error}')\n",
    "            print(f'‚úì Model loaded successfully from ultralytics cache')\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f'\\n‚ùå Error downloading model: {e}')\n",
    "        raise\n",
    "else:\n",
    "    model = YOLO(str(model_path))\n",
    "    print(f'‚úì Model loaded from {model_path}')\n",
    "\n",
    "# Get model information\n",
    "model_params = sum(p.numel() for p in model.model.parameters())\n",
    "model_size_mb = model_path.stat().st_size / (1024*1024) if model_path.exists() else 0\n",
    "\n",
    "# Calculate FLOPs using model.info() method which is more accurate\n",
    "try:\n",
    "    # FLOPs for 640x640 input (standard YOLO input size)\n",
    "    flops = get_flops(model.model, imgsz=(1, 3, 640, 640))\n",
    "    flops_gflops = flops / 1e9\n",
    "except Exception as e:\n",
    "    print(f'‚ö†Ô∏è  Could not calculate FLOPs: {e}')\n",
    "    flops_gflops = 0\n",
    "\n",
    "print(f'\\nüìä Model Information:')\n",
    "print(f'  Model: {MODEL_NAME}')\n",
    "print(f'  Classes in model: {len(model.names)}')\n",
    "print(f'  Task: {model.task}')\n",
    "print(f'  Parameters: {model_params / 1e6:.1f}M')\n",
    "print(f'  Model Size: {model_size_mb:.1f} MB')\n",
    "print(f'  FLOPs (640x640): {flops_gflops:.2f} GFLOPs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f25df55",
   "metadata": {},
   "source": [
    "## 4. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c63218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all images and labels\n",
    "image_files = sorted(list(IMAGES_DIR.glob('*.jpg')) + list(IMAGES_DIR.glob('*.png')))\n",
    "label_files = sorted([LABELS_DIR / f'{img.stem}.txt' for img in image_files if (LABELS_DIR / f'{img.stem}.txt').exists()])\n",
    "\n",
    "# Filter to only images with labels\n",
    "valid_images = [img for img in image_files if (LABELS_DIR / f'{img.stem}.txt').exists()]\n",
    "\n",
    "print(f'‚úì Dataset loaded')\n",
    "print(f'  Total images: {len(image_files)}')\n",
    "print(f'  Images with labels: {len(valid_images)}')\n",
    "print(f'  Label files: {len(label_files)}')\n",
    "\n",
    "# Load performance metadata for per-image attribute analysis\n",
    "METADATA_DIR = YOLO_DATASET_ROOT / 'representative_json'\n",
    "PERFORMANCE_FILE = METADATA_DIR / f'{USED_DATA_SPLIT}_performance_analysis.json'\n",
    "\n",
    "if PERFORMANCE_FILE.exists():\n",
    "    with open(PERFORMANCE_FILE, 'r') as f:\n",
    "        performance_data = json.load(f)\n",
    "    \n",
    "    print(f'\\n‚úì Performance metadata loaded: {PERFORMANCE_FILE.name}')\n",
    "    print(f'  Images with attributes: {performance_data[\"total_images\"]}')\n",
    "    print(f'  Attributes available: weather, scene, timeofday')\n",
    "    print(f'  Per-image class distribution available')\n",
    "    \n",
    "    # Create quick lookup dictionary for attributes\n",
    "    image_attributes = {img['basename']: img for img in performance_data['images']}\n",
    "else:\n",
    "    print(f'\\n‚ö†Ô∏è  Performance metadata not found: {PERFORMANCE_FILE}')\n",
    "    print(f'  Attribute-based analysis will not be available')\n",
    "    print(f'  Run: python3 process_bdd100k_to_yolo_dataset.py')\n",
    "    performance_data = None\n",
    "    image_attributes = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552ca01b",
   "metadata": {},
   "source": [
    "## 6. Run Official YOLO Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1251769",
   "metadata": {},
   "source": [
    "### IMPORTANT: Optimized Validation Approach\n",
    "This notebook uses **YOLO's official validation method** to calculate all metrics, confusion matrix, and predictions in a single pass. This approach:\n",
    "- ‚úÖ **Faster**: Single validation pass instead of multiple loops\n",
    "- ‚úÖ **No Duplicates**: Uses YOLO's built-in validation logic\n",
    "- ‚úÖ **Official Metrics**: Provides mAP, precision, recall directly from YOLO\n",
    "- ‚úÖ **Confusion Matrix**: Extracted from YOLO validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8ec6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run official YOLO validation with W&B tracking\n",
    "print('=' * 80)\n",
    "print('RUNNING OFFICIAL YOLO VALIDATION WITH W&B TRACKING')\n",
    "print('=' * 80)\n",
    "\n",
    "# Create a dataset structure that YOLO expects\n",
    "validation_dataset_root = TMP_DIR / 'yolo_validation_dataset'\n",
    "validation_images_dir = validation_dataset_root / 'images' / USED_DATA_SPLIT\n",
    "validation_labels_dir = validation_dataset_root / 'labels' / USED_DATA_SPLIT\n",
    "\n",
    "# Create directories\n",
    "validation_images_dir.mkdir(parents=True, exist_ok=True)\n",
    "validation_labels_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create symbolic links to images and labels\n",
    "print(f'Setting up validation dataset structure...')\n",
    "\n",
    "# Link images\n",
    "for img_file in IMAGES_DIR.glob('*.jpg'):\n",
    "    link_path = validation_images_dir / img_file.name\n",
    "    if not link_path.exists():\n",
    "        try:\n",
    "            link_path.symlink_to(img_file)\n",
    "        except:\n",
    "            shutil.copy2(img_file, link_path)\n",
    "\n",
    "for img_file in IMAGES_DIR.glob('*.png'):\n",
    "    link_path = validation_images_dir / img_file.name\n",
    "    if not link_path.exists():\n",
    "        try:\n",
    "            link_path.symlink_to(img_file)\n",
    "        except:\n",
    "            shutil.copy2(img_file, link_path)\n",
    "\n",
    "# Link labels\n",
    "for label_file in LABELS_DIR.glob('*.txt'):\n",
    "    if label_file.name != 'classes.txt':\n",
    "        link_path = validation_labels_dir / label_file.name\n",
    "        if not link_path.exists():\n",
    "            try:\n",
    "                link_path.symlink_to(label_file)\n",
    "            except:\n",
    "                shutil.copy2(label_file, link_path)\n",
    "\n",
    "print(f'‚úì Validation dataset prepared')\n",
    "print(f'  Images: {len(list(validation_images_dir.glob(\"*\")))}')\n",
    "print(f'  Labels: {len(list(validation_labels_dir.glob(\"*.txt\")))}')\n",
    "\n",
    "# Create data.yaml file\n",
    "data_yaml_path = validation_dataset_root / 'data.yaml'\n",
    "iou_threshold = 0.5\n",
    "data_yaml_content = f\"\"\"path: {validation_dataset_root}\n",
    "train: images/train\n",
    "val: images/{USED_DATA_SPLIT}\n",
    "test: images/test\n",
    "\n",
    "nc: {NUM_CLASSES}\n",
    "names: {list(CLASS_NAMES.values())}\n",
    "\"\"\"\n",
    "\n",
    "with open(data_yaml_path, 'w') as f:\n",
    "    f.write(data_yaml_content)\n",
    "\n",
    "print(f'‚úì Created data.yaml at: {data_yaml_path}')\n",
    "\n",
    "# Initialize Weights & Biases\n",
    "try:\n",
    "    import wandb\n",
    "    wandb.init(\n",
    "        project=\"yolo-bdd100k-validation\",\n",
    "        name=W_B_RUN_NAME,\n",
    "        config={\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"dataset\": USED_DATASET,\n",
    "            \"split\": USED_DATA_SPLIT,\n",
    "            \"iou_threshold\": iou_threshold,\n",
    "            \"num_classes\": NUM_CLASSES,\n",
    "            \"model_params\": model_params,\n",
    "            \"model_size_mb\": model_size_mb,\n",
    "            \"flops_gflops\": flops_gflops\n",
    "        }\n",
    "    )\n",
    "    print(f'\\n‚úì Weights & Biases initialized: {W_B_RUN_NAME}')\n",
    "    print(f'  Project: yolo-bdd100k-validation')\n",
    "    print(f'  Run: {W_B_RUN_NAME}')\n",
    "except ImportError:\n",
    "    print('\\n‚ö†Ô∏è  Weights & Biases not available. Install with: pip install wandb')\n",
    "    wandb = None\n",
    "except Exception as e:\n",
    "    print(f'\\n‚ö†Ô∏è  W&B initialization error: {e}')\n",
    "    print('  Continuing without W&B tracking...')\n",
    "    wandb = None\n",
    "\n",
    "# Run validation with timing\n",
    "print('\\nRunning YOLO validation...')\n",
    "start_time = time.time()\n",
    "\n",
    "validation_results = model.val(\n",
    "    data=str(data_yaml_path),\n",
    "    split=USED_DATA_SPLIT,\n",
    "    device=device,\n",
    "    save_json=False,\n",
    "    save_txt=False,\n",
    "    conf=0.001,\n",
    "    iou=iou_threshold,\n",
    "    verbose=True,\n",
    "    plots=True,\n",
    "    project=str(RUN_DIR),\n",
    "    name='yolo_validation'\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "# Extract speed metrics from YOLO validation\n",
    "num_images = len(list(validation_images_dir.glob(\"*.jpg\"))) + len(list(validation_images_dir.glob(\"*.png\")))\n",
    "avg_inference_time = total_time / num_images if num_images > 0 else 0\n",
    "fps = 1 / avg_inference_time if avg_inference_time > 0 else 0\n",
    "\n",
    "# Extract GFLOPs from validation results if available\n",
    "if hasattr(validation_results, 'speed') and hasattr(validation_results.speed, 'flops'):\n",
    "    flops_gflops = validation_results.speed.flops / 1e9\n",
    "    print(f'\\n‚úì GFLOPs extracted from validation results: {flops_gflops:.2f} GFLOPs')\n",
    "elif hasattr(model, 'info'):\n",
    "    # Fallback: use model.info() if validation doesn't have GFLOPs\n",
    "    try:\n",
    "        model_info = model.info(verbose=False)\n",
    "        if hasattr(model_info, 'flops'):\n",
    "            flops_gflops = model_info.flops / 1e9\n",
    "            print(f'\\n‚úì GFLOPs extracted from model info: {flops_gflops:.2f} GFLOPs')\n",
    "    except:\n",
    "        print(f'\\n‚úì Using pre-calculated GFLOPs: {flops_gflops:.2f} GFLOPs')\n",
    "else:\n",
    "    print(f'\\n‚úì Using pre-calculated GFLOPs: {flops_gflops:.2f} GFLOPs')\n",
    "\n",
    "# Extract overall metrics\n",
    "yolo_metrics = {\n",
    "    'precision': float(validation_results.box.mp),\n",
    "    'recall': float(validation_results.box.mr),\n",
    "    'map50': float(validation_results.box.map50),\n",
    "    'map50_95': float(validation_results.box.map),\n",
    "    'fitness': float(validation_results.fitness)\n",
    "}\n",
    "\n",
    "# Extract per-class metrics from YOLO\n",
    "yolo_class_metrics = {}\n",
    "class_tp = {}\n",
    "class_fp = {}\n",
    "class_fn = {}\n",
    "\n",
    "if hasattr(validation_results.box, 'ap_class_index') and len(validation_results.box.ap_class_index) > 0:\n",
    "    for i, class_idx in enumerate(validation_results.box.ap_class_index):\n",
    "        class_idx = int(class_idx)\n",
    "        class_name = CLASS_NAMES.get(class_idx, f'class_{class_idx}')\n",
    "        \n",
    "        precision = float(validation_results.box.p[i]) if i < len(validation_results.box.p) else 0.0\n",
    "        recall = float(validation_results.box.r[i]) if i < len(validation_results.box.r) else 0.0\n",
    "        ap50 = float(validation_results.box.ap50[i]) if i < len(validation_results.box.ap50) else 0.0\n",
    "        ap50_95 = float(validation_results.box.ap[i]) if i < len(validation_results.box.ap) else 0.0\n",
    "        \n",
    "        yolo_class_metrics[class_name] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'ap50': ap50,\n",
    "            'ap50_95': ap50_95\n",
    "        }\n",
    "        \n",
    "        class_tp[class_idx] = 0\n",
    "        class_fp[class_idx] = 0\n",
    "        class_fn[class_idx] = 0\n",
    "\n",
    "# Extract confusion matrix from YOLO validation results\n",
    "confusion_matrix = validation_results.confusion_matrix.matrix if hasattr(validation_results, 'confusion_matrix') else np.zeros((NUM_CLASSES, NUM_CLASSES), dtype=int)\n",
    "\n",
    "# Verify confusion matrix exists\n",
    "if confusion_matrix is not None and confusion_matrix.size > 0:\n",
    "    print(f'\\n‚úì Confusion matrix extracted successfully')\n",
    "    print(f'  Shape: {confusion_matrix.shape}')\n",
    "    print(f'  Diagonal sum (correct): {np.trace(confusion_matrix)}')\n",
    "else:\n",
    "    print(f'\\n‚ö†Ô∏è  Warning: Confusion matrix is empty')\n",
    "\n",
    "# Calculate TP, FP, FN from confusion matrix\n",
    "for i in range(NUM_CLASSES):\n",
    "    class_tp[i] = int(confusion_matrix[i, i]) if i < confusion_matrix.shape[0] and i < confusion_matrix.shape[1] else 0\n",
    "    \n",
    "    if i < confusion_matrix.shape[1]:\n",
    "        class_fp[i] = int(confusion_matrix[:, i].sum() - confusion_matrix[i, i])\n",
    "    else:\n",
    "        class_fp[i] = 0\n",
    "    \n",
    "    if i < confusion_matrix.shape[0]:\n",
    "        class_fn[i] = int(confusion_matrix[i, :].sum() - confusion_matrix[i, i])\n",
    "    else:\n",
    "        class_fn[i] = 0\n",
    "\n",
    "# Create results_data for compatibility with report generation\n",
    "results_data = []\n",
    "for img_path in validation_images_dir.glob(\"*.jpg\"):\n",
    "    results_data.append({'image_path': img_path})\n",
    "for img_path in validation_images_dir.glob(\"*.png\"):\n",
    "    results_data.append({'image_path': img_path})\n",
    "\n",
    "print('\\n' + '=' * 80)\n",
    "print('OFFICIAL YOLO VALIDATION RESULTS')\n",
    "print('=' * 80)\n",
    "print(f\"Precision (mean): {yolo_metrics['precision']:.4f}\")\n",
    "print(f\"Recall (mean):    {yolo_metrics['recall']:.4f}\")\n",
    "print(f\"mAP@0.5:          {yolo_metrics['map50']:.4f}\")\n",
    "print(f\"mAP@0.5:0.95:     {yolo_metrics['map50_95']:.4f}\")\n",
    "print(f\"Fitness:          {yolo_metrics['fitness']:.4f}\")\n",
    "print(f'\\n‚ö° Performance Metrics:')\n",
    "print(f'  Total Time: {total_time:.2f}s')\n",
    "print(f'  Average Inference Time: {avg_inference_time*1000:.2f}ms per image')\n",
    "print(f'  FPS (Frames Per Second): {fps:.2f}')\n",
    "print('=' * 80)\n",
    "\n",
    "# Log comprehensive metrics to Weights & Biases\n",
    "if wandb:\n",
    "    try:\n",
    "        # Log overall metrics\n",
    "        wandb.log({\n",
    "            # Accuracy metrics\n",
    "            \"metrics/precision\": yolo_metrics['precision'],\n",
    "            \"metrics/recall\": yolo_metrics['recall'],\n",
    "            \"metrics/mAP@0.5\": yolo_metrics['map50'],\n",
    "            \"metrics/mAP@0.5:0.95\": yolo_metrics['map50_95'],\n",
    "            \"metrics/fitness\": yolo_metrics['fitness'],\n",
    "            \n",
    "            # Performance metrics\n",
    "            \"performance/total_time_seconds\": total_time,\n",
    "            \"performance/avg_inference_time_ms\": avg_inference_time * 1000,\n",
    "            \"performance/fps\": fps,\n",
    "            \"performance/images_processed\": num_images,\n",
    "            \n",
    "            # Model info\n",
    "            \"model/parameters_millions\": model_params / 1e6,\n",
    "            \"model/size_mb\": model_size_mb,\n",
    "            \"model/flops_gflops\": flops_gflops\n",
    "        })\n",
    "        \n",
    "        # Log per-class metrics\n",
    "        for class_name, metrics in yolo_class_metrics.items():\n",
    "            wandb.log({\n",
    "                f\"class/{class_name}/precision\": metrics['precision'],\n",
    "                f\"class/{class_name}/recall\": metrics['recall'],\n",
    "                f\"class/{class_name}/ap50\": metrics['ap50'],\n",
    "                f\"class/{class_name}/ap50_95\": metrics['ap50_95']\n",
    "            })\n",
    "        \n",
    "        print(f'\\n‚úì Metrics logged to Weights & Biases')\n",
    "        print(f'  Dashboard: https://wandb.ai/yolo-bdd100k-validation/{W_B_RUN_NAME}')\n",
    "    except Exception as e:\n",
    "        print(f'\\n‚ö†Ô∏è  Error logging to W&B: {e}')\n",
    "\n",
    "# Save YOLO validation results with performance metrics\n",
    "yolo_results_path = RUN_DIR / 'yolo_validation_metrics.json'\n",
    "with open(yolo_results_path, 'w') as f:\n",
    "    json.dump({\n",
    "        'overall_metrics': yolo_metrics,\n",
    "        'per_class_metrics': yolo_class_metrics,\n",
    "        'performance': {\n",
    "            'total_time_seconds': float(total_time),\n",
    "            'avg_inference_time_ms': float(avg_inference_time * 1000),\n",
    "            'fps': float(fps),\n",
    "            'images_processed': int(num_images)\n",
    "        },\n",
    "        'model_info': {\n",
    "            'parameters': int(model_params),\n",
    "            'model_size_mb': float(model_size_mb),\n",
    "            'flops_gflops': float(flops_gflops)\n",
    "        }\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f'\\n‚úì YOLO validation results saved to: {yolo_results_path}')\n",
    "print(f'‚úì YOLO validation plots saved to: {RUN_DIR / \"yolo_validation\"}')\n",
    "print(f'‚úì Confusion matrix extracted and will be visualized in next cell')\n",
    "print(f'  Diagonal sum (correct predictions): {np.trace(confusion_matrix)}')\n",
    "print(f'  Total predictions: {confusion_matrix.sum()}')\n",
    "\n",
    "# Finish W&B run\n",
    "if wandb:\n",
    "    try:\n",
    "        wandb.finish()\n",
    "        print(f'\\n‚úì Weights & Biases run completed successfully')\n",
    "    except Exception as e:\n",
    "        print(f'\\n‚ö†Ô∏è  Error finishing W&B run: {e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60078000",
   "metadata": {},
   "source": [
    "## 7. Display Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addec244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-class metrics from YOLO validation results\n",
    "metrics_data = []\n",
    "\n",
    "for class_id in sorted(CLASS_NAMES.keys()):\n",
    "    tp = class_tp.get(class_id, 0)\n",
    "    fp = class_fp.get(class_id, 0)\n",
    "    fn = class_fn.get(class_id, 0)\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    map50 = yolo_class_metrics.get(CLASS_NAMES[class_id], {}).get('ap50', 0.0)\n",
    "\n",
    "    metrics_data.append({\n",
    "        'Class': CLASS_NAMES[class_id],\n",
    "        'TP': tp,\n",
    "        'FP': fp,\n",
    "        'FN': fn,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'mAP@0.5': map50\n",
    "    })\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics_data)\n",
    "\n",
    "# Overall metrics\n",
    "total_tp = sum(class_tp.values())\n",
    "total_fp = sum(class_fp.values())\n",
    "total_fn = sum(class_fn.values())\n",
    "\n",
    "overall_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "overall_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "overall_f1 = 2 * overall_precision * overall_recall / (overall_precision + overall_recall) if (overall_precision + overall_recall) > 0 else 0\n",
    "\n",
    "print('=' * 80)\n",
    "print('YOLOv8 VALIDATION RESULTS SUMMARY')\n",
    "print('=' * 80)\n",
    "print(f'\\nDataset: {USED_DATASET} - {USED_DATA_SPLIT} split')\n",
    "print(f'Images processed: {len(list(validation_images_dir.glob(\"*\")))}')\n",
    "print(f'IoU Threshold: {iou_threshold}')\n",
    "print(f'\\nOVERALL METRICS (From YOLO Validation):')\n",
    "print(f'  Precision: {yolo_metrics[\"precision\"]:.4f}')\n",
    "print(f'  Recall:    {yolo_metrics[\"recall\"]:.4f}')\n",
    "print(f'  mAP@0.5:   {yolo_metrics[\"map50\"]:.4f}')\n",
    "print(f'  mAP@0.5:0.95: {yolo_metrics[\"map50_95\"]:.4f}')\n",
    "print(f'\\nOVERALL METRICS (From Confusion Matrix):')\n",
    "print(f'  Precision: {overall_precision:.4f}')\n",
    "print(f'  Recall:    {overall_recall:.4f}')\n",
    "print(f'  F1-Score:  {overall_f1:.4f}')\n",
    "print(f'\\nPER-CLASS METRICS (including mAP@0.5):')\n",
    "print(df_metrics.to_string(index=False))\n",
    "print('\\n' + '=' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccba6091",
   "metadata": {},
   "source": [
    "## 8. Visualize Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76936a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 1: Core Metrics (Precision, Recall, F1-Score)\n",
    "fig1, axes1 = plt.subplots(2, 2, figsize=(18, 12))\n",
    "ax_precision, ax_recall, ax_f1, ax_counts = axes1.flatten()\n",
    "\n",
    "# Precision by class\n",
    "precision_sorted = df_metrics.sort_values('Precision')\n",
    "ax_precision.barh(precision_sorted['Class'], precision_sorted['Precision'], color='#5BC0EB')\n",
    "ax_precision.set_title('Precision by Class', fontweight='bold', fontsize=16)\n",
    "ax_precision.set_xlabel('Precision', fontweight='bold')\n",
    "ax_precision.set_xlim(0, 1)\n",
    "ax_precision.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Recall by class\n",
    "recall_sorted = df_metrics.sort_values('Recall')\n",
    "ax_recall.barh(recall_sorted['Class'], recall_sorted['Recall'], color='#F25F5C')\n",
    "ax_recall.set_title('Recall by Class', fontweight='bold', fontsize=16)\n",
    "ax_recall.set_xlabel('Recall', fontweight='bold')\n",
    "ax_recall.set_xlim(0, 1)\n",
    "ax_recall.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# F1-score by class\n",
    "f1_sorted = df_metrics.sort_values('F1-Score')\n",
    "ax_f1.barh(f1_sorted['Class'], f1_sorted['F1-Score'], color='#9BC53D')\n",
    "ax_f1.set_title('F1-Score by Class', fontweight='bold', fontsize=16)\n",
    "ax_f1.set_xlabel('F1-Score', fontweight='bold')\n",
    "ax_f1.set_xlim(0, 1)\n",
    "ax_f1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Ground truth object counts / detections distribution\n",
    "ax_counts.bar(['TP', 'FP', 'FN'], [total_tp, total_fp, total_fn], color=['#177E89', '#ED6A5A', '#F4A259'])\n",
    "ax_counts.set_title('Overall Detection Outcomes', fontweight='bold', fontsize=16)\n",
    "ax_counts.set_ylabel('Count', fontweight='bold')\n",
    "for bar in ax_counts.patches:\n",
    "    ax_counts.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(total_tp, total_fp, total_fn)*0.01,\n",
    "                   f'{int(bar.get_height())}', ha='center', fontweight='bold')\n",
    "ax_counts.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save Figure 1\n",
    "metrics_fig_path = RUN_DIR / 'core_metrics_charts.png'\n",
    "plt.savefig(metrics_fig_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'‚úì Core metrics visualization saved: {metrics_fig_path}')\n",
    "\n",
    "# Figure 2: mAP Metrics\n",
    "fig2, axes2 = plt.subplots(1, 2, figsize=(18, 6))\n",
    "ax_map, ax_overall = axes2.flatten()\n",
    "\n",
    "# mAP@0.5 by class\n",
    "map_sorted = df_metrics.sort_values('mAP@0.5')\n",
    "ax_map.barh(map_sorted['Class'], map_sorted['mAP@0.5'], color='#B388EB')\n",
    "ax_map.set_title('mAP@0.5 by Class', fontweight='bold', fontsize=16)\n",
    "ax_map.set_xlabel('mAP@0.5', fontweight='bold')\n",
    "ax_map.set_xlim(0, 1)\n",
    "ax_map.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Overall metrics bar chart (including mAP metrics)\n",
    "overall_plot_values = {\n",
    "    'Precision': overall_precision,\n",
    "    'Recall': overall_recall,\n",
    "    'F1-Score': overall_f1,\n",
    "    'mAP@0.5': yolo_metrics['map50'],\n",
    "    'mAP@0.5:0.95': yolo_metrics['map50_95']\n",
    "}\n",
    "ax_overall.bar(overall_plot_values.keys(), overall_plot_values.values(), color='#FFA630')\n",
    "ax_overall.set_ylim(0, 1)\n",
    "ax_overall.set_title('Overall Metrics', fontweight='bold', fontsize=16)\n",
    "ax_overall.set_ylabel('Score', fontweight='bold')\n",
    "for idx, value in enumerate(overall_plot_values.values()):\n",
    "    ax_overall.text(idx, value + 0.02, f'{value:.3f}', ha='center', fontweight='bold')\n",
    "ax_overall.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save Figure 2\n",
    "map_fig_path = RUN_DIR / 'map_metrics_charts.png'\n",
    "plt.savefig(map_fig_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'‚úì mAP metrics visualization saved: {map_fig_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602394be",
   "metadata": {},
   "source": [
    "## 8.5. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a90f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix from YOLO validation - centered and compact\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Draw each cell manually with solid colors\n",
    "for i in range(NUM_CLASSES):\n",
    "    for j in range(NUM_CLASSES):\n",
    "        value = confusion_matrix[i, j]\n",
    "        \n",
    "        # Determine cell color\n",
    "        if value == 0:\n",
    "            # White for empty cells\n",
    "            cell_color = 'white'\n",
    "        elif i == j:\n",
    "            cell_color = '#00A676'  # Correct predictions\n",
    "        else:\n",
    "            cell_color = '#D7263D'  # Misclassifications\n",
    "        rect = Rectangle((j - 0.5, i - 0.5), 1, 1,\n",
    "                         facecolor=cell_color,\n",
    "                         edgecolor='black',\n",
    "                         linewidth=1.5)\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        # Add text annotations with smaller font\n",
    "        if value > 0:\n",
    "            text_color = 'white' if i == j else '#F7F7F7'\n",
    "            ax.text(j, i, str(value), ha='center', va='center',\n",
    "                    color=text_color, fontsize=9, fontweight='bold')\n",
    "\n",
    "# Set axis limits and properties\n",
    "ax.set_xlim(-0.5, NUM_CLASSES - 0.5)\n",
    "ax.set_ylim(NUM_CLASSES - 0.5, -0.5)\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# Set ticks and labels with smaller font\n",
    "class_labels = [CLASS_NAMES[i] for i in range(NUM_CLASSES)]\n",
    "ax.set_xticks(np.arange(NUM_CLASSES))\n",
    "ax.set_yticks(np.arange(NUM_CLASSES))\n",
    "ax.set_xticklabels(class_labels, fontsize=8, fontweight='bold', rotation=45, ha='right')\n",
    "ax.set_yticklabels(class_labels, fontsize=8, fontweight='bold')\n",
    "ax.set_xlabel('Predicted Class', fontweight='bold', fontsize=11)\n",
    "ax.set_ylabel('True Class', fontweight='bold', fontsize=11)\n",
    "ax.set_title(f'Confusion Matrix ({MODEL_NAME} validation)', fontweight='bold', fontsize=13)\n",
    "ax.grid(False)\n",
    "\n",
    "# Center the confusion matrix in the figure\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save confusion matrix figure for PDF report\n",
    "confusion_matrix_path = RUN_DIR / 'confusion_matrix.png'\n",
    "plt.savefig(confusion_matrix_path, dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"(Green = Correct Predictions, Red = Incorrect Predictions, White = No Predictions)\")\n",
    "print(f'‚úì Confusion matrix visualized (from {MODEL_NAME} validation)')\n",
    "print(f'  Diagonal sum (correct predictions): {np.trace(confusion_matrix)}')\n",
    "print(f'  Total predictions: {confusion_matrix.sum()}')\n",
    "print(f'  Saved to: {confusion_matrix_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3361edcf",
   "metadata": {},
   "source": [
    "## 9. Detailed Comparison: Ground Truth vs Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66a2262",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons_dir = RUN_DIR / 'sample_comparisons'\n",
    "comparisons_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def draw_ground_truth(img_path: Path, label_path: Path,\n",
    "                      class_names: Dict[int, str], \n",
    "                      colors: Dict[int, Tuple[int, int, int]]) -> Tuple[np.ndarray, int]:\n",
    "    \"\"\"Draw ground-truth boxes using deterministic colors.\n",
    "    \n",
    "    Args:\n",
    "        img_path: Path to the image file\n",
    "        label_path: Path to the YOLO format label file\n",
    "        class_names: Mapping from class ID to class name\n",
    "        colors: Mapping from class ID to BGR color tuple\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (annotated RGB image, object count)\n",
    "    \"\"\"\n",
    "    img_bgr = cv2.imread(str(img_path))\n",
    "    if img_bgr is None:\n",
    "        raise FileNotFoundError(f'Image not found: {img_path}')\n",
    "    h, w = img_bgr.shape[:2]\n",
    "    object_count = 0\n",
    "\n",
    "    if label_path.exists():\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    object_count += 1\n",
    "                    class_id = int(parts[0])\n",
    "                    x_center, y_center, width, height = map(float, parts[1:5])\n",
    "                    x1 = int((x_center - width / 2) * w)\n",
    "                    y1 = int((y_center - height / 2) * h)\n",
    "                    x2 = int((x_center + width / 2) * w)\n",
    "                    y2 = int((y_center + height / 2) * h)\n",
    "                    color = tuple(int(c) for c in colors.get(class_id, (255, 255, 255)))\n",
    "                    cv2.rectangle(img_bgr, (x1, y1), (x2, y2), color, 3)\n",
    "                    label = class_names.get(class_id, f'class_{class_id}')\n",
    "                    (label_w, label_h), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n",
    "                    cv2.rectangle(img_bgr,\n",
    "                                  (x1, max(0, y1 - label_h - baseline - 6)),\n",
    "                                  (x1 + label_w + 8, y1), color, -1)\n",
    "                    cv2.putText(img_bgr, label, (x1 + 4, y1 - 4), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                                0.7, (0, 0, 0) if sum(color) > 500 else (255, 255, 255), 2)\n",
    "\n",
    "    return cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB), object_count\n",
    "\n",
    "def draw_predictions_with_consistent_colors(result: Any, \n",
    "                                           colors: Dict[int, Tuple[int, int, int]],\n",
    "                                           class_names: Dict[int, str]) -> np.ndarray:\n",
    "    \"\"\"Draw model predictions using same palette as ground truth.\n",
    "    \n",
    "    Args:\n",
    "        result: YOLO prediction result object\n",
    "        colors: Mapping from class ID to BGR color tuple\n",
    "        class_names: Mapping from class ID to class name\n",
    "    \n",
    "    Returns:\n",
    "        Annotated RGB image\n",
    "    \"\"\"\n",
    "    img_bgr = result.orig_img.copy()\n",
    "    if img_bgr.ndim == 2:\n",
    "        img_bgr = cv2.cvtColor(img_bgr, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    for box in result.boxes:\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "        conf = float(box.conf[0])\n",
    "        class_id = int(box.cls[0])\n",
    "        color = tuple(int(c) for c in colors.get(class_id, (255, 255, 255)))\n",
    "        cv2.rectangle(img_bgr, (x1, y1), (x2, y2), color, 3)\n",
    "        label = f\"{class_names.get(class_id, f'class_{class_id}')} {conf:.2f}\"\n",
    "        (label_w, label_h), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n",
    "        cv2.rectangle(img_bgr,\n",
    "                      (x1, max(0, y1 - label_h - baseline - 6)),\n",
    "                      (x1 + label_w + 8, y1), color, -1)\n",
    "        cv2.putText(img_bgr, label, (x1 + 4, y1 - 4), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.7, (0, 0, 0) if sum(color) > 500 else (255, 255, 255), 2)\n",
    "\n",
    "    return cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def add_attributes_text(img_rgb: np.ndarray, attrs: Dict[str, Any]) -> np.ndarray:\n",
    "    \"\"\"Overlay weather/scene/time attributes at the bottom of the image.\n",
    "    \n",
    "    Args:\n",
    "        img_rgb: Input RGB image\n",
    "        attrs: Dictionary containing attribute information\n",
    "    \n",
    "    Returns:\n",
    "        RGB image with overlaid attribute text\n",
    "    \"\"\"\n",
    "    if not attrs:\n",
    "        return img_rgb\n",
    "    \n",
    "    img_with_text = img_rgb.copy()\n",
    "    h, w = img_with_text.shape[:2]\n",
    "    \n",
    "    # Create text overlay\n",
    "    weather = attrs.get('weather', 'unknown')\n",
    "    scene = attrs.get('scene', 'unknown')\n",
    "    timeofday = attrs.get('timeofday', 'unknown')\n",
    "    attr_text = f\"Weather: {weather} | Scene: {scene} | Time: {timeofday}\"\n",
    "    \n",
    "    # Convert to BGR for OpenCV\n",
    "    img_bgr = cv2.cvtColor(img_with_text, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Add semi-transparent background for text\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.6\n",
    "    thickness = 2\n",
    "    (text_w, text_h), baseline = cv2.getTextSize(attr_text, font, font_scale, thickness)\n",
    "    \n",
    "    # Position at bottom of image\n",
    "    text_x = 10\n",
    "    text_y = h - 15\n",
    "    \n",
    "    # Draw background rectangle\n",
    "    overlay = img_bgr.copy()\n",
    "    cv2.rectangle(overlay, (0, h - text_h - baseline - 20), (w, h), (0, 0, 0), -1)\n",
    "    cv2.addWeighted(overlay, 0.6, img_bgr, 0.4, 0, img_bgr)\n",
    "    \n",
    "    # Draw text\n",
    "    cv2.putText(img_bgr, attr_text, (text_x, text_y), font, font_scale, (255, 255, 255), thickness)\n",
    "    \n",
    "    return cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def render_comparison(idx: int, img_path: Path,\n",
    "                     gt_img: np.ndarray, gt_count: int,\n",
    "                     pred_img: np.ndarray, pred_count: int,\n",
    "                     attrs: Dict[str, Any]) -> plt.Figure:\n",
    "    \"\"\"Render side-by-side comparison figure.\n",
    "    \n",
    "    Args:\n",
    "        idx: Comparison index\n",
    "        img_path: Path to the original image\n",
    "        gt_img: Ground truth annotated image (RGB)\n",
    "        gt_count: Number of ground truth objects\n",
    "        pred_img: Prediction annotated image (RGB)\n",
    "        pred_count: Number of predicted objects\n",
    "        attrs: Image attributes (weather, scene, time)\n",
    "    \n",
    "    Returns:\n",
    "        Matplotlib figure object\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    \n",
    "    ax1.imshow(gt_img)\n",
    "    ax1.set_title(f'Ground Truth ({gt_count} objects)', fontweight='bold', fontsize=14)\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    ax2.imshow(pred_img)\n",
    "    ax2.set_title(f'Prediction ({pred_count} objects)', fontweight='bold', fontsize=14)\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    # Overall title with environmental context\n",
    "    weather = attrs.get('weather', 'unknown')\n",
    "    scene = attrs.get('scene', 'unknown')\n",
    "    timeofday = attrs.get('timeofday', 'unknown')\n",
    "    fig.suptitle(\n",
    "        f'Comparison #{idx}: {img_path.name}\\n{weather} | {scene} | {timeofday}',\n",
    "        fontsize=16,\n",
    "        fontweight='bold'\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def analyze_comparison(gt_count: int, det_count: int, \n",
    "                      result: Any, attrs: Dict[str, Any],\n",
    "                      class_names: Dict[int, str]) -> str:\n",
    "    \"\"\"Generate textual analysis for a comparison.\n",
    "    \n",
    "    Args:\n",
    "        gt_count: Number of ground truth objects\n",
    "        det_count: Number of detected objects\n",
    "        result: YOLO prediction result\n",
    "        attrs: Image attributes\n",
    "        class_names: Mapping from class ID to class name\n",
    "    \n",
    "    Returns:\n",
    "        Analysis string\n",
    "    \"\"\"\n",
    "    analysis = []\n",
    "    \n",
    "    # Detection summary\n",
    "    if det_count == gt_count:\n",
    "        analysis.append(f\"‚úì Perfect count match: {det_count} objects detected (same as ground truth)\")\n",
    "    elif det_count > gt_count:\n",
    "        analysis.append(f\"‚ö†Ô∏è Over-detection: {det_count} objects detected vs {gt_count} ground truth (difference: +{det_count - gt_count})\")\n",
    "    else:\n",
    "        analysis.append(f\"‚ö†Ô∏è Under-detection: {det_count} objects detected vs {gt_count} ground truth (difference: -{gt_count - det_count})\")\n",
    "    \n",
    "    # Class distribution\n",
    "    if result.boxes:\n",
    "        class_counts = {}\n",
    "        all_confs = []\n",
    "        for box in result.boxes:\n",
    "            cls_id = int(box.cls[0])\n",
    "            cls_name = class_names.get(cls_id, f'class_{cls_id}')\n",
    "            class_counts[cls_name] = class_counts.get(cls_name, 0) + 1\n",
    "            all_confs.append(float(box.conf[0]))\n",
    "        \n",
    "        class_dist = ', '.join([f'{k}={v}' for k, v in sorted(class_counts.items())])\n",
    "        analysis.append(f\"üìä Detected classes: {class_dist}\")\n",
    "        \n",
    "        avg_conf = sum(all_confs) / len(all_confs) if all_confs else 0\n",
    "        min_conf = min(all_confs) if all_confs else 0\n",
    "        max_conf = max(all_confs) if all_confs else 0\n",
    "        analysis.append(f\"üéØ Confidence: avg={avg_conf:.2f}, min={min_conf:.2f}, max={max_conf:.2f}\")\n",
    "    else:\n",
    "        analysis.append(\"‚ö†Ô∏è No objects detected in this image\")\n",
    "    \n",
    "    # Environmental context\n",
    "    if attrs:\n",
    "        weather = attrs.get('weather', 'unknown')\n",
    "        scene = attrs.get('scene', 'unknown')\n",
    "        timeofday = attrs.get('timeofday', 'unknown')\n",
    "        analysis.append(f\"üåç Context: {weather} weather, {scene}, {timeofday}\")\n",
    "        \n",
    "        # Performance insights based on conditions\n",
    "        if timeofday == 'night' and det_count < gt_count:\n",
    "            analysis.append(\"üí° Lower detection in night conditions - typical challenge for vision models\")\n",
    "        elif weather in ['rainy', 'snowy'] and det_count < gt_count:\n",
    "            analysis.append(\"üí° Adverse weather may affect detection performance\")\n",
    "    \n",
    "    return '\\n'.join(analysis)\n",
    "\n",
    "num_comparisons = min(12, len(valid_images))\n",
    "comparison_image_paths = []\n",
    "comparison_analyses = []  # Store analyses for report\n",
    "\n",
    "if num_comparisons == 0:\n",
    "    print('‚ö†Ô∏è  No labeled images available for comparison generation.')\n",
    "else:\n",
    "    sample_images = random.sample(valid_images, num_comparisons) if len(valid_images) > num_comparisons else valid_images\n",
    "    print(f'Generating {len(sample_images)} comparison figures...')\n",
    "    sample_results = []\n",
    "    for img_path in tqdm(sample_images, desc='Running inference for comparisons'):\n",
    "        result = model(str(img_path), verbose=False, device=device)[0]\n",
    "        sample_results.append({'image_path': img_path,\n",
    "                               'result': result,\n",
    "                               'num_detections': len(result.boxes)})\n",
    "\n",
    "    for idx, sample_data in enumerate(sample_results, 1):\n",
    "        img_path = sample_data['image_path']\n",
    "        label_path = LABELS_DIR / f'{img_path.stem}.txt'\n",
    "        attrs = image_attributes.get(img_path.stem, {})\n",
    "\n",
    "        gt_img, gt_count = draw_ground_truth(img_path, label_path, CLASS_NAMES, COLORS)\n",
    "        gt_img = add_attributes_text(gt_img, attrs)\n",
    "\n",
    "        pred_img = draw_predictions_with_consistent_colors(sample_data['result'], COLORS, CLASS_NAMES)\n",
    "        pred_img = add_attributes_text(pred_img, attrs)\n",
    "\n",
    "        fig = render_comparison(idx, img_path, gt_img, gt_count,\n",
    "                                pred_img, sample_data['num_detections'], attrs)\n",
    "        comparison_path = comparisons_dir / f'comparison_{idx:02d}.png'\n",
    "        plt.savefig(comparison_path, dpi=150, bbox_inches='tight')\n",
    "        comparison_image_paths.append(comparison_path)\n",
    "        plt.show()\n",
    "        \n",
    "        # Generate and display analysis for this comparison\n",
    "        analysis = analyze_comparison(gt_count, sample_data['num_detections'], \n",
    "                                      sample_data['result'], attrs, CLASS_NAMES)\n",
    "        print(f'\\nüìù Analysis for Comparison #{idx}:')\n",
    "        print('-' * 80)\n",
    "        print(analysis)\n",
    "        print('-' * 80)\n",
    "        \n",
    "        # Store analysis for report\n",
    "        comparison_analyses.append({\n",
    "            'comparison_id': idx,\n",
    "            'image_name': img_path.name,\n",
    "            'analysis': analysis\n",
    "        })\n",
    "        \n",
    "        plt.close(fig)\n",
    "    \n",
    "    # Save all comparison analyses to a text file\n",
    "    analyses_file = comparisons_dir / 'comparison_analyses.txt'\n",
    "    with open(analyses_file, 'w') as f:\n",
    "        f.write('YOLO VALIDATION - COMPARISON ANALYSES\\n')\n",
    "        f.write('=' * 80 + '\\n')\n",
    "        f.write(f'Model: {MODEL_NAME}\\n')\n",
    "        f.write(f'Dataset: {USED_DATASET} - {USED_DATA_SPLIT} split\\n')\n",
    "        f.write(f'Generated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\\n')\n",
    "        f.write('=' * 80 + '\\n\\n')\n",
    "        \n",
    "        for comp in comparison_analyses:\n",
    "            f.write(f'Comparison #{comp[\"comparison_id\"]}: {comp[\"image_name\"]}\\n')\n",
    "            f.write('-' * 80 + '\\n')\n",
    "            f.write(comp['analysis'] + '\\n')\n",
    "            f.write('\\n' + '=' * 80 + '\\n\\n')\n",
    "\n",
    "    print('=' * 80)\n",
    "    print(f'‚úì Generated {len(comparison_image_paths)} individual comparison figures')\n",
    "    print(f'  Saved to: {comparisons_dir}')\n",
    "    print(f'‚úì Comparison analyses saved to: {analyses_file.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66da26d3",
   "metadata": {},
   "source": [
    "## 10. Attribute-Based Performance Analysis\n",
    "\n",
    "Analyze model performance across different environmental conditions and scenes using the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaa975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if performance_data and image_attributes:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ATTRIBUTE-BASED PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    results_by_weather: Dict[str, list] = {}\n",
    "    results_by_scene: Dict[str, list] = {}\n",
    "    results_by_timeofday: Dict[str, list] = {}\n",
    "    results_by_class: Dict[str, list] = {}\n",
    "\n",
    "    for result in results_data:\n",
    "        basename = Path(result['image_path']).stem\n",
    "        attrs = image_attributes.get(basename, {})\n",
    "        weather = attrs.get('weather', 'unknown')\n",
    "        scene = attrs.get('scene', 'unknown')\n",
    "        timeofday = attrs.get('timeofday', 'unknown')\n",
    "        classes_present = attrs.get('classes_present', [])\n",
    "\n",
    "        results_by_weather.setdefault(weather, []).append(result)\n",
    "        results_by_scene.setdefault(scene, []).append(result)\n",
    "        results_by_timeofday.setdefault(timeofday, []).append(result)\n",
    "        for cls in classes_present:\n",
    "            results_by_class.setdefault(cls, []).append(result)\n",
    "\n",
    "    def calculate_group_metrics(group_results):\n",
    "        \"\"\"Calculate metrics for a group of results using per-class confusion matrix data\"\"\"\n",
    "        if not group_results:\n",
    "            return None\n",
    "        total_tp = total_fp = total_fn = 0\n",
    "        for result in group_results:\n",
    "            basename = Path(result['image_path']).stem\n",
    "            attrs = image_attributes.get(basename, {})\n",
    "            classes_present = attrs.get('classes_present', [])\n",
    "            objects_per_class = attrs.get('objects_per_class', {})\n",
    "            for class_name in classes_present:\n",
    "                class_id = CLASS_NAME_TO_ID.get(class_name)\n",
    "                if class_id is None:\n",
    "                    continue\n",
    "                tp = class_tp.get(class_id, 0)\n",
    "                fp = class_fp.get(class_id, 0)\n",
    "                fn = class_fn.get(class_id, 0)\n",
    "                obj_count = objects_per_class.get(class_name, 1)\n",
    "                total_tp += tp * obj_count\n",
    "                total_fp += fp * obj_count\n",
    "                total_fn += fn * obj_count\n",
    "        precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n",
    "        recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        return {\n",
    "            'count': len(group_results),\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'tp': total_tp,\n",
    "            'fp': total_fp,\n",
    "            'fn': total_fn\n",
    "        }\n",
    "\n",
    "    def print_group_stats(title, metrics_dict):\n",
    "        \"\"\"Print statistics for a group of metrics organized by some attribute\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(title)\n",
    "        print(\"=\" * 80)\n",
    "        for key, group in sorted(metrics_dict.items()):\n",
    "            metrics = calculate_group_metrics(group)\n",
    "            if not metrics:\n",
    "                continue\n",
    "            print(f\"\\n{key.upper()}:\")\n",
    "            print(f\"  Images: {metrics['count']}\")\n",
    "            print(f\"  Precision: {metrics['precision']:.3f} (TP={metrics['tp']}, FP={metrics['fp']})\")\n",
    "            print(f\"  Recall: {metrics['recall']:.3f} (TP={metrics['tp']}, FN={metrics['fn']})\")\n",
    "            print(f\"  F1 Score: {metrics['f1']:.3f}\")\n",
    "\n",
    "    print_group_stats(\"PERFORMANCE BY WEATHER CONDITION\", results_by_weather)\n",
    "    print_group_stats(\"PERFORMANCE BY SCENE TYPE\", results_by_scene)\n",
    "    print_group_stats(\"PERFORMANCE BY TIME OF DAY\", results_by_timeofday)\n",
    "\n",
    "    class_metrics = {}\n",
    "    for cls, group in results_by_class.items():\n",
    "        metrics = calculate_group_metrics(group)\n",
    "        if metrics:\n",
    "            class_metrics[cls] = metrics\n",
    "    top_classes = sorted(class_metrics.items(), key=lambda x: x[1]['count'], reverse=True)[:10]\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"PERFORMANCE BY CLASS (Top 10 by image count)\")\n",
    "    print(\"=\" * 80)\n",
    "    for cls, metrics in top_classes:\n",
    "        print(f\"\\n{cls.upper()}:\")\n",
    "        print(f\"  Images: {metrics['count']}\")\n",
    "        print(f\"  Precision: {metrics['precision']:.3f}\")\n",
    "        print(f\"  Recall: {metrics['recall']:.3f}\")\n",
    "        print(f\"  F1 Score: {metrics['f1']:.3f}\")\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "    def plot_horizontal(ax, labels, values, title, cmap):\n",
    "        \"\"\"Plot horizontal bar chart for attribute-based performance\"\"\"\n",
    "        if not labels:\n",
    "            ax.text(0.5, 0.5, 'No data', ha='center', va='center')\n",
    "            ax.set_axis_off()\n",
    "            return\n",
    "        bars = ax.barh(labels, values, color=cmap(np.linspace(0.4, 0.8, len(labels))))\n",
    "        for bar, value in zip(bars, values):\n",
    "            ax.text(value + 0.02, bar.get_y() + bar.get_height() / 2,\n",
    "                    f'{value:.3f}', va='center', fontsize=9)\n",
    "        ax.set_xlim(0, 1.05)\n",
    "        ax.set_xlabel('F1 Score')\n",
    "        ax.set_title(title, fontweight='bold')\n",
    "\n",
    "    weather_labels = list(results_by_weather.keys())\n",
    "    weather_values = [calculate_group_metrics(results_by_weather[w])['f1'] for w in weather_labels]\n",
    "    plot_horizontal(axes[0, 0], weather_labels, weather_values, 'Performance by Weather', plt.cm.Blues)\n",
    "\n",
    "    scene_labels = list(results_by_scene.keys())\n",
    "    scene_values = [calculate_group_metrics(results_by_scene[s])['f1'] for s in scene_labels]\n",
    "    plot_horizontal(axes[0, 1], scene_labels, scene_values, 'Performance by Scene', plt.cm.Greens)\n",
    "\n",
    "    time_labels = list(results_by_timeofday.keys())\n",
    "    time_values = [calculate_group_metrics(results_by_timeofday[t])['f1'] for t in time_labels]\n",
    "    plot_horizontal(axes[1, 0], time_labels, time_values, 'Performance by Time of Day', plt.cm.Oranges)\n",
    "\n",
    "    class_labels = [c[0] for c in top_classes[:8]]\n",
    "    class_values = [c[1]['f1'] for c in top_classes[:8]]\n",
    "    plot_horizontal(axes[1, 1], class_labels, class_values, 'Performance by Class (Top 8)', plt.cm.Purples)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    attribute_perf_path = RUN_DIR / 'attribute_performance.png'\n",
    "    plt.savefig(attribute_perf_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n‚úì Saved attribute performance visualization:\", attribute_perf_path)\n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚úì ATTRIBUTE-BASED ANALYSIS COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Performance metadata not loaded\")\n",
    "    print(\"‚ö†Ô∏è  Attribute-based analysis not available\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2c5d7c",
   "metadata": {},
   "source": [
    "# 11. Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4998c8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive PDF report with all visualizations\n",
    "pdf_report_path = RUN_DIR / 'report.pdf'\n",
    "\n",
    "# Create PDF document\n",
    "doc = SimpleDocTemplate(str(pdf_report_path), pagesize=A4,\n",
    "                       rightMargin=30, leftMargin=30,\n",
    "                       topMargin=30, bottomMargin=30)\n",
    "\n",
    "# Container for PDF elements\n",
    "story = []\n",
    "styles = getSampleStyleSheet()\n",
    "\n",
    "# Custom styles\n",
    "title_style = ParagraphStyle(\n",
    "    'CustomTitle',\n",
    "    parent=styles['Heading1'],\n",
    "    fontSize=24,\n",
    "    textColor=colors.HexColor('#2c3e50'),\n",
    "    spaceAfter=30,\n",
    "    alignment=TA_CENTER\n",
    ")\n",
    "\n",
    "heading_style = ParagraphStyle(\n",
    "    'CustomHeading',\n",
    "    parent=styles['Heading2'],\n",
    "    fontSize=16,\n",
    "    textColor=colors.HexColor('#34495e'),\n",
    "    spaceAfter=12,\n",
    "    spaceBefore=20\n",
    ")\n",
    "\n",
    "# Title\n",
    "story.append(Paragraph('YOLO Validation Report', title_style))\n",
    "story.append(Spacer(1, 12))\n",
    "\n",
    "# Info section with model details\n",
    "info_data = [\n",
    "    ['Model:', MODEL_NAME],\n",
    "    ['Model Size:', f'{model_size_mb:.1f} MB'],\n",
    "    ['Parameters:', f'{model_params / 1e6:.1f}M'],\n",
    "    ['FLOPs (640x640):', f'{flops_gflops:.2f} GFLOPs'],\n",
    "    ['Run Name:', RUN_NAME],\n",
    "    ['W&B Run Name:', W_B_RUN_NAME],\n",
    "    ['Timestamp:', datetime.now().strftime('%Y-%m-%d %H:%M:%S')],\n",
    "    ['Dataset:', f'{USED_DATASET} - {USED_DATA_SPLIT} split'],\n",
    "    ['Images Processed:', str(len(results_data))],\n",
    "    ['IoU Threshold:', str(iou_threshold)]\n",
    "]\n",
    "\n",
    "info_table = Table(info_data, colWidths=[2.2*inch, 3.8*inch])\n",
    "info_table.setStyle(TableStyle([\n",
    "    ('BACKGROUND', (0, 0), (-1, -1), colors.HexColor('#ecf0f1')),\n",
    "    ('TEXTCOLOR', (0, 0), (-1, -1), colors.black),\n",
    "    ('ALIGN', (0, 0), (-1, -1), 'LEFT'),\n",
    "    ('FONTNAME', (0, 0), (0, -1), 'Helvetica-Bold'),\n",
    "    ('FONTSIZE', (0, 0), (-1, -1), 10),\n",
    "    ('BOTTOMPADDING', (0, 0), (-1, -1), 8),\n",
    "    ('TOPPADDING', (0, 0), (-1, -1), 8),\n",
    "    ('GRID', (0, 0), (-1, -1), 1, colors.white)\n",
    "]))\n",
    "story.append(info_table)\n",
    "story.append(Spacer(1, 20))\n",
    "\n",
    "# Performance metrics section\n",
    "story.append(Paragraph('Inference Performance', heading_style))\n",
    "perf_data = [\n",
    "    ['Metric', 'Value'],\n",
    "    ['Total Execution Time', f'{total_time:.2f}s'],\n",
    "    ['Average Inference Time', f'{avg_inference_time*1000:.2f}ms per image'],\n",
    "    ['FPS (Frames Per Second)', f'{fps:.2f}']\n",
    "]\n",
    "\n",
    "perf_table = Table(perf_data, colWidths=[3*inch, 3*inch])\n",
    "perf_table.setStyle(TableStyle([\n",
    "    ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#27ae60')),\n",
    "    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n",
    "    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n",
    "    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
    "    ('FONTSIZE', (0, 0), (-1, 0), 12),\n",
    "    ('FONTSIZE', (0, 1), (-1, -1), 10),\n",
    "    ('BOTTOMPADDING', (0, 0), (-1, -1), 8),\n",
    "    ('TOPPADDING', (0, 0), (-1, -1), 8),\n",
    "    ('BACKGROUND', (0, 1), (-1, -1), colors.HexColor('#d5f4e6')),\n",
    "    ('GRID', (0, 0), (-1, -1), 1, colors.black)\n",
    "]))\n",
    "story.append(perf_table)\n",
    "story.append(Spacer(1, 20))\n",
    "\n",
    "# Overall accuracy metrics\n",
    "story.append(Paragraph('Overall Accuracy Metrics', heading_style))\n",
    "acc_data = [\n",
    "    ['Metric', 'Value'],\n",
    "    ['Precision', f'{overall_precision:.4f}'],\n",
    "    ['Recall', f'{overall_recall:.4f}'],\n",
    "    ['F1-Score', f'{overall_f1:.4f}'],\n",
    "    ['mAP@0.5', f'{yolo_metrics[\"map50\"]:.4f}'],\n",
    "    ['mAP@0.5:0.95', f'{yolo_metrics[\"map50_95\"]:.4f}'],\n",
    "    ['True Positives', str(total_tp)],\n",
    "    ['False Positives', str(total_fp)],\n",
    "    ['False Negatives', str(total_fn)]\n",
    "]\n",
    "\n",
    "acc_table = Table(acc_data, colWidths=[3*inch, 3*inch])\n",
    "acc_table.setStyle(TableStyle([\n",
    "    ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#3498db')),\n",
    "    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n",
    "    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n",
    "    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
    "    ('FONTSIZE', (0, 0), (-1, 0), 12),\n",
    "    ('FONTSIZE', (0, 1), (-1, -1), 10),\n",
    "    ('BOTTOMPADDING', (0, 0), (-1, -1), 8),\n",
    "    ('TOPPADDING', (0, 0), (-1, -1), 8),\n",
    "    ('BACKGROUND', (0, 1), (-1, -1), colors.beige),\n",
    "    ('GRID', (0, 0), (-1, -1), 1, colors.black)\n",
    "]))\n",
    "story.append(acc_table)\n",
    "story.append(Spacer(1, 20))\n",
    "\n",
    "# Add metrics visualizations to PDF\n",
    "story.append(PageBreak())\n",
    "story.append(Paragraph('Performance Visualizations', heading_style))\n",
    "story.append(Spacer(1, 12))\n",
    "\n",
    "# Core Metrics Chart (Precision, Recall, F1, Detection Outcomes)\n",
    "story.append(Paragraph('Core Metrics (Precision, Recall, F1-Score)', ParagraphStyle('SubHeading', parent=styles['Normal'], fontSize=12, textColor=colors.HexColor('#34495e'), spaceAfter=8, spaceBefore=8)))\n",
    "core_metrics_path = RUN_DIR / 'core_metrics_charts.png'\n",
    "if core_metrics_path.exists():\n",
    "    try:\n",
    "        with PILImage.open(core_metrics_path) as img:\n",
    "            img_width, img_height = img.size\n",
    "            aspect_ratio = img_height / img_width\n",
    "            pdf_width = 7 * inch\n",
    "            pdf_height = pdf_width * aspect_ratio\n",
    "            story.append(Image(str(core_metrics_path), width=pdf_width, height=pdf_height))\n",
    "    except Exception as e:\n",
    "        print(f'Warning: Could not load core metrics chart with PIL: {e}')\n",
    "        story.append(Image(str(core_metrics_path), width=7*inch, height=5*inch))\n",
    "else:\n",
    "    story.append(Paragraph('Core metrics chart not found.', styles['Normal']))\n",
    "\n",
    "story.append(Spacer(1, 12))\n",
    "\n",
    "# mAP Metrics Chart\n",
    "story.append(Paragraph('mAP Metrics', ParagraphStyle('SubHeading', parent=styles['Normal'], fontSize=12, textColor=colors.HexColor('#34495e'), spaceAfter=8, spaceBefore=8)))\n",
    "map_metrics_path = RUN_DIR / 'map_metrics_charts.png'\n",
    "if map_metrics_path.exists():\n",
    "    try:\n",
    "        with PILImage.open(map_metrics_path) as img:\n",
    "            img_width, img_height = img.size\n",
    "            aspect_ratio = img_height / img_width\n",
    "            pdf_width = 7 * inch\n",
    "            pdf_height = pdf_width * aspect_ratio\n",
    "            story.append(Image(str(map_metrics_path), width=pdf_width, height=pdf_height))\n",
    "    except Exception as e:\n",
    "        print(f'Warning: Could not load mAP metrics chart with PIL: {e}')\n",
    "        story.append(Image(str(map_metrics_path), width=7*inch, height=5*inch))\n",
    "else:\n",
    "    story.append(Paragraph('mAP metrics chart not found.', styles['Normal']))\n",
    "\n",
    "story.append(Spacer(1, 20))\n",
    "\n",
    "# Per-class metrics table\n",
    "story.append(PageBreak())\n",
    "story.append(Paragraph('Per-Class Performance', heading_style))\n",
    "\n",
    "table_data = [['Class', 'TP', 'FP', 'FN', 'Precision', 'Recall', 'F1-Score', 'mAP@0.5']]\n",
    "for _, row in df_metrics.iterrows():\n",
    "    class_name = row['Class']\n",
    "    map50_val = yolo_class_metrics.get(class_name, {}).get('ap50', 0.0)\n",
    "    table_data.append([\n",
    "        row['Class'],\n",
    "        str(row['TP']),\n",
    "        str(row['FP']),\n",
    "        str(row['FN']),\n",
    "        f\"{row['Precision']:.4f}\",\n",
    "        f\"{row['Recall']:.4f}\",\n",
    "        f\"{row['F1-Score']:.4f}\",\n",
    "        f\"{map50_val:.4f}\"\n",
    "    ])\n",
    "\n",
    "per_class_table = Table(table_data, colWidths=[1.0*inch, 0.5*inch, 0.5*inch, 0.5*inch, 0.8*inch, 0.8*inch, 0.8*inch, 0.8*inch])\n",
    "per_class_table.setStyle(TableStyle([\n",
    "    ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#3498db')),\n",
    "    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n",
    "    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n",
    "    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),\n",
    "    ('FONTSIZE', (0, 0), (-1, 0), 8),\n",
    "    ('FONTSIZE', (0, 1), (-1, -1), 7),\n",
    "    ('BOTTOMPADDING', (0, 0), (-1, -1), 6),\n",
    "    ('TOPPADDING', (0, 0), (-1, -1), 6),\n",
    "    ('ROWBACKGROUNDS', (0, 1), (-1, -1), [colors.white, colors.lightgrey]),\n",
    "    ('GRID', (0, 0), (-1, -1), 1, colors.black)\n",
    "]))\n",
    "story.append(per_class_table)\n",
    "story.append(Spacer(1, 20))\n",
    "\n",
    "# Confusion Matrix\n",
    "story.append(PageBreak())\n",
    "story.append(Paragraph('Confusion Matrix', heading_style))\n",
    "story.append(Paragraph(f'Correct Predictions (Diagonal Sum): {np.trace(confusion_matrix)}', styles['Normal']))\n",
    "story.append(Paragraph(f'Total Matched Predictions: {confusion_matrix.sum()}', styles['Normal']))\n",
    "story.append(Spacer(1, 12))\n",
    "\n",
    "confusion_matrix_img_path = RUN_DIR / 'confusion_matrix.png'\n",
    "if confusion_matrix_img_path.exists():\n",
    "    try:\n",
    "        with PILImage.open(confusion_matrix_img_path) as img:\n",
    "            img_width, img_height = img.size\n",
    "            aspect_ratio = img_height / img_width\n",
    "            pdf_width = 6.5 * inch\n",
    "            pdf_height = pdf_width * aspect_ratio\n",
    "            if pdf_height > 7 * inch:\n",
    "                pdf_height = 7 * inch\n",
    "                pdf_width = pdf_height / aspect_ratio\n",
    "            story.append(Image(str(confusion_matrix_img_path), width=pdf_width, height=pdf_height))\n",
    "    except Exception as e:\n",
    "        print(f'Warning: Could not load confusion matrix with PIL: {e}')\n",
    "        story.append(Image(str(confusion_matrix_img_path), width=6.5*inch, height=6.5*inch))\n",
    "else:\n",
    "    story.append(Paragraph('Confusion matrix image not found.', styles['Normal']))\n",
    "\n",
    "story.append(Spacer(1, 20))\n",
    "\n",
    "# Sample predictions comparison - multiple images per page\n",
    "comparisons_dir = RUN_DIR / 'sample_comparisons'\n",
    "if comparisons_dir.exists():\n",
    "    comparison_files = sorted(comparisons_dir.glob('comparison_*.png'))\n",
    "    \n",
    "    if comparison_files:\n",
    "        story.append(PageBreak())\n",
    "        story.append(Paragraph('Sample Predictions: Ground Truth vs Predictions', heading_style))\n",
    "        story.append(Paragraph(f'{len(comparison_files)} detailed side-by-side comparisons', styles['Normal']))\n",
    "        story.append(Spacer(1, 12))\n",
    "        \n",
    "        # Add comparison images - 2 per page with page breaks between pairs\n",
    "        for i, comparison_path in enumerate(comparison_files):\n",
    "            # Add page break after every 3 images (except before the first)\n",
    "            if i > 0 and i % 3 == 0:\n",
    "                story.append(PageBreak())\n",
    "            \n",
    "            try:\n",
    "                with PILImage.open(comparison_path) as img:\n",
    "                    img_width, img_height = img.size\n",
    "                    aspect_ratio = img_height / img_width\n",
    "                    \n",
    "                    # Smaller width to fit 2 per page\n",
    "                    pdf_width = 7 * inch\n",
    "                    pdf_height = pdf_width * aspect_ratio\n",
    "                    \n",
    "                    # Limit height to fit 2 images per page\n",
    "                    if pdf_height > 3.5 * inch:\n",
    "                        pdf_height = 3.5 * inch\n",
    "                        pdf_width = pdf_height / aspect_ratio\n",
    "                    \n",
    "                    story.append(Image(str(comparison_path), width=pdf_width, height=pdf_height))\n",
    "                    story.append(Spacer(1, 8))\n",
    "            except Exception as e:\n",
    "                print(f'Warning: Could not load {comparison_path.name} with PIL: {e}')\n",
    "                story.append(Image(str(comparison_path), width=7*inch, height=3*inch))\n",
    "                story.append(Spacer(1, 8))\n",
    "    else:\n",
    "        story.append(PageBreak())\n",
    "        story.append(Paragraph('Sample Predictions: Ground Truth vs Predictions', heading_style))\n",
    "        story.append(Paragraph('No comparison images found.', styles['Normal']))\n",
    "else:\n",
    "    story.append(PageBreak())\n",
    "    story.append(Paragraph('Sample Predictions: Ground Truth vs Predictions', heading_style))\n",
    "    story.append(Paragraph('Comparison directory not found.', styles['Normal']))\n",
    "\n",
    "story.append(Spacer(1, 12))\n",
    "story.append(Paragraph('Additional validation plots available in: yolo_validation folder', styles['Normal']))\n",
    "\n",
    "# Footer\n",
    "story.append(Spacer(1, 30))\n",
    "story.append(Paragraph('Generated by YOLO Quick Test Notebook', \n",
    "                      ParagraphStyle('Footer', parent=styles['Normal'], \n",
    "                                   alignment=TA_CENTER, textColor=colors.grey)))\n",
    "story.append(Paragraph('BDD100K Dataset - Computer Vision Project', \n",
    "                      ParagraphStyle('Footer2', parent=styles['Normal'], \n",
    "                                   alignment=TA_CENTER, textColor=colors.grey)))\n",
    "\n",
    "# Build PDF\n",
    "doc.build(story)\n",
    "\n",
    "# Generate JSON file with comprehensive comparison data\n",
    "json_report_path = RUN_DIR / 'metrics_data.json'\n",
    "\n",
    "comparison_data = {\n",
    "    'metadata': {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'run_name': RUN_NAME,\n",
    "        'wb_run_name': W_B_RUN_NAME,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'dataset': USED_DATASET,\n",
    "        'data_split': USED_DATA_SPLIT,\n",
    "        'images_processed': len(results_data),\n",
    "        'iou_threshold': iou_threshold,\n",
    "        'num_classes': NUM_CLASSES\n",
    "    },\n",
    "    'model_info': {\n",
    "        'parameters': int(model_params),\n",
    "        'model_size_mb': float(model_size_mb),\n",
    "        'flops_gflops': float(flops_gflops)\n",
    "    },\n",
    "    'performance': {\n",
    "        'total_time_seconds': float(total_time),\n",
    "        'avg_inference_time_ms': float(avg_inference_time * 1000),\n",
    "        'fps': float(fps),\n",
    "        'images_processed': int(len(results_data))\n",
    "    },\n",
    "    'custom_metrics': {\n",
    "        'overall': {\n",
    "            'precision': float(overall_precision),\n",
    "            'recall': float(overall_recall),\n",
    "            'f1_score': float(overall_f1),\n",
    "            'true_positives': int(total_tp),\n",
    "            'false_positives': int(total_fp),\n",
    "            'false_negatives': int(total_fn)\n",
    "        },\n",
    "        'per_class': {}\n",
    "    },\n",
    "    'yolo_official_metrics': {\n",
    "        'overall': yolo_metrics,\n",
    "        'per_class': yolo_class_metrics\n",
    "    },\n",
    "    'confusion_matrix': {\n",
    "        'matrix': confusion_matrix.tolist(),\n",
    "        'diagonal_sum': int(np.trace(confusion_matrix)),\n",
    "        'total_predictions': int(confusion_matrix.sum())\n",
    "    },\n",
    "    'class_names': CLASS_NAMES\n",
    "}\n",
    "\n",
    "# Add per-class custom metrics\n",
    "for _, row in df_metrics.iterrows():\n",
    "    class_name = row['Class']\n",
    "    comparison_data['custom_metrics']['per_class'][class_name] = {\n",
    "        'true_positives': int(row['TP']),\n",
    "        'false_positives': int(row['FP']),\n",
    "        'false_negatives': int(row['FN']),\n",
    "        'precision': float(row['Precision']),\n",
    "        'recall': float(row['Recall']),\n",
    "        'f1_score': float(row['F1-Score'])\n",
    "    }\n",
    "\n",
    "# Save JSON file\n",
    "with open(json_report_path, 'w') as f:\n",
    "    json.dump(comparison_data, f, indent=2)\n",
    "\n",
    "# Count comparison images\n",
    "comparisons_dir = RUN_DIR / 'sample_comparisons'\n",
    "num_comparison_images = len(list(comparisons_dir.glob('comparison_*.png'))) if comparisons_dir.exists() else 0\n",
    "analyses_file_exists = (comparisons_dir / 'comparison_analyses.txt').exists() if comparisons_dir.exists() else False\n",
    "\n",
    "# Summary output\n",
    "print('=' * 80)\n",
    "print('‚úì COMPREHENSIVE REPORT GENERATED')\n",
    "print('=' * 80)\n",
    "print(f'\\nAll outputs saved to: {RUN_DIR}')\n",
    "print(f'\\nGenerated files:')\n",
    "print(f'  üìÑ PDF Report: report.pdf ({pdf_report_path.stat().st_size / 1024:.2f} KB)')\n",
    "print(f'  üìä Metrics JSON: metrics_data.json ({json_report_path.stat().st_size / 1024:.2f} KB)')\n",
    "print(f'  üìà YOLO Validation: yolo_validation_metrics.json')\n",
    "print(f'  üñºÔ∏è  Confusion Matrix: confusion_matrix.png')\n",
    "print(f'  üìä Core Metrics Chart: core_metrics_charts.png')\n",
    "print(f'  üìä mAP Metrics Chart: map_metrics_charts.png')\n",
    "print(f'  üñºÔ∏è  Sample Comparisons: sample_comparisons/ ({num_comparison_images} images)')\n",
    "if analyses_file_exists:\n",
    "    print(f'  üìù Comparison Analyses: sample_comparisons/comparison_analyses.txt')\n",
    "print(f'  üìÅ YOLO Outputs: yolo_validation/ (plots, curves, etc.)')\n",
    "print(f'\\nReport contents:')\n",
    "print(f'  - Model Information (Size: {model_size_mb:.1f}MB, Params: {model_params/1e6:.1f}M, FLOPs: {flops_gflops:.2f}G)')\n",
    "print(f'  - Performance Metrics (FPS: {fps:.2f}, Avg: {avg_inference_time*1000:.2f}ms)')\n",
    "print(f'  - Overall accuracy metrics (Precision, Recall, F1, mAP)')\n",
    "print(f'  - Per-class performance with IoU metrics')\n",
    "print(f'  - Core metrics visualization (Precision, Recall, F1, Detection Outcomes)')\n",
    "print(f'  - mAP metrics visualization (mAP@0.5, Overall Metrics)')\n",
    "print(f'  - Confusion matrix visualization')\n",
    "print(f'  - {num_comparison_images} individual side-by-side prediction comparisons (full-width, high-resolution)')\n",
    "if analyses_file_exists:\n",
    "    print(f'  - Detailed analyses for each comparison (saved to text file)')\n",
    "print(f'  - Custom & YOLO official metrics')\n",
    "print(f'\\nüíæ Metrics saved to JSON for future comparison')\n",
    "\n",
    "print(f'üìä Weights & Biases tracking: {W_B_RUN_NAME}')\n",
    "print('=' * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolo_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
